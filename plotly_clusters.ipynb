{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74aa5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "from textwrap import wrap\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0557db18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>alignment</th>\n",
       "      <th>social aspect</th>\n",
       "      <th>modality</th>\n",
       "      <th>type</th>\n",
       "      <th>subtopic</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kravchenko-etal-2025-ualign</td>\n",
       "      <td>{UA}lign: {LLM} Alignment Benchmark for the {U...</td>\n",
       "      <td>This paper introduces UAlign, the comprehensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>moral</td>\n",
       "      <td>While \"moral\" and \"ethical\" have slightly diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liu-etal-2025-smaller</td>\n",
       "      <td>Smaller Large Language Models Can Do Moral Sel...</td>\n",
       "      <td>Self-correction is one of the most amazing eme...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation;Mitigation</td>\n",
       "      <td>moral;safety</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yu-etal-2025-diverse</td>\n",
       "      <td>Diverse {AI} Feedback For Large Language Model...</td>\n",
       "      <td>Recent advances in large language models (LLMs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chen-etal-2025-instructioncp</td>\n",
       "      <td>{I}nstruction{CP}: A Simple yet Effective Appr...</td>\n",
       "      <td>The rapid development of large language models...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation;Mitigation</td>\n",
       "      <td>safety;multilingual</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mittal-etal-2025-protect</td>\n",
       "      <td>{PROTECT}: Policy-Related Organizational Value...</td>\n",
       "      <td>This paper presents PROTECT, a novel policy-dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation</td>\n",
       "      <td>ethical</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>yao-etal-2024-value</td>\n",
       "      <td>Value {FULCRA}: Mapping Large Language Models ...</td>\n",
       "      <td>Value alignment is crucial for the responsible...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training</td>\n",
       "      <td>value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>jones-etal-2024-multi</td>\n",
       "      <td>A Multi-Aspect Framework for Counter Narrative...</td>\n",
       "      <td>Counter narratives - informed responses to hat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>toxicity</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>zhan-etal-2024-removing</td>\n",
       "      <td>Removing {RLHF} Protections in {GPT}-4 via Fin...</td>\n",
       "      <td>As large language models (LLMs) have increased...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>safety</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>du-etal-2024-zhujiu</td>\n",
       "      <td>{Z}hu{J}iu-Knowledge: A Fairer Platform for Ev...</td>\n",
       "      <td>The swift advancement in large language models...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>hu-etal-2024-language</td>\n",
       "      <td>Language Models are Alignable Decision-Makers:...</td>\n",
       "      <td>In difficult decision-making scenarios, it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training</td>\n",
       "      <td>ethical</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ID  \\\n",
       "0     kravchenko-etal-2025-ualign   \n",
       "1           liu-etal-2025-smaller   \n",
       "2            yu-etal-2025-diverse   \n",
       "3    chen-etal-2025-instructioncp   \n",
       "4        mittal-etal-2025-protect   \n",
       "..                            ...   \n",
       "286           yao-etal-2024-value   \n",
       "287         jones-etal-2024-multi   \n",
       "288       zhan-etal-2024-removing   \n",
       "289           du-etal-2024-zhujiu   \n",
       "290         hu-etal-2024-language   \n",
       "\n",
       "                                                 title  \\\n",
       "0    {UA}lign: {LLM} Alignment Benchmark for the {U...   \n",
       "1    Smaller Large Language Models Can Do Moral Sel...   \n",
       "2    Diverse {AI} Feedback For Large Language Model...   \n",
       "3    {I}nstruction{CP}: A Simple yet Effective Appr...   \n",
       "4    {PROTECT}: Policy-Related Organizational Value...   \n",
       "..                                                 ...   \n",
       "286  Value {FULCRA}: Mapping Large Language Models ...   \n",
       "287  A Multi-Aspect Framework for Counter Narrative...   \n",
       "288  Removing {RLHF} Protections in {GPT}-4 via Fin...   \n",
       "289  {Z}hu{J}iu-Knowledge: A Fairer Platform for Ev...   \n",
       "290  Language Models are Alignable Decision-Makers:...   \n",
       "\n",
       "                                              abstract  alignment  \\\n",
       "0    This paper introduces UAlign, the comprehensiv...          1   \n",
       "1    Self-correction is one of the most amazing eme...          1   \n",
       "2    Recent advances in large language models (LLMs...          1   \n",
       "3    The rapid development of large language models...          1   \n",
       "4    This paper presents PROTECT, a novel policy-dr...          1   \n",
       "..                                                 ...        ...   \n",
       "286  Value alignment is crucial for the responsible...          1   \n",
       "287  Counter narratives - informed responses to hat...          1   \n",
       "288  As large language models (LLMs) have increased...          1   \n",
       "289  The swift advancement in large language models...          1   \n",
       "290  In difficult decision-making scenarios, it is ...          1   \n",
       "\n",
       "     social aspect modality                            type  \\\n",
       "0                1     Text                      Evaluation   \n",
       "1                1     Text           Evaluation;Mitigation   \n",
       "2                1     Text             Training;Evaluation   \n",
       "3                1     Text  Training;Evaluation;Mitigation   \n",
       "4                1     Text             Training;Evaluation   \n",
       "..             ...      ...                             ...   \n",
       "286              1     Text                        Training   \n",
       "287              1     Text                      Evaluation   \n",
       "288              1     Text                      Evaluation   \n",
       "289              1     Text                      Evaluation   \n",
       "290              1     Text                        Training   \n",
       "\n",
       "                subtopic                                              notes  \n",
       "0                  moral  While \"moral\" and \"ethical\" have slightly diff...  \n",
       "1           moral;safety                                                NaN  \n",
       "2                General                                                NaN  \n",
       "3    safety;multilingual                                                NaN  \n",
       "4                ethical                                                NaN  \n",
       "..                   ...                                                ...  \n",
       "286                value                                                NaN  \n",
       "287             toxicity                                                NaN  \n",
       "288               safety                                                NaN  \n",
       "289              General                                                NaN  \n",
       "290              ethical                                                NaN  \n",
       "\n",
       "[281 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"sociotechnical_alignment_papers.csv\"\n",
    "df = pd.read_csv(filename, sep=';', keep_default_na=True)\n",
    "df = df.dropna(subset=['subtopic'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78129caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "for item in df['ID']:\n",
    "    obj = re.search('^.+?\\-((?:20)\\d{2})\\-', item)\n",
    "    year = obj.group(1)\n",
    "    years.append(year)\n",
    "if 'year' not in df.columns:\n",
    "    df.insert(loc=1, column='year', value=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e446d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>alignment</th>\n",
       "      <th>social aspect</th>\n",
       "      <th>modality</th>\n",
       "      <th>type</th>\n",
       "      <th>subtopic</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kravchenko-etal-2025-ualign</td>\n",
       "      <td>2025</td>\n",
       "      <td>{UA}lign: {LLM} Alignment Benchmark for the {U...</td>\n",
       "      <td>This paper introduces UAlign, the comprehensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>moral</td>\n",
       "      <td>While \"moral\" and \"ethical\" have slightly diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liu-etal-2025-smaller</td>\n",
       "      <td>2025</td>\n",
       "      <td>Smaller Large Language Models Can Do Moral Sel...</td>\n",
       "      <td>Self-correction is one of the most amazing eme...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation;Mitigation</td>\n",
       "      <td>moral;safety</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yu-etal-2025-diverse</td>\n",
       "      <td>2025</td>\n",
       "      <td>Diverse {AI} Feedback For Large Language Model...</td>\n",
       "      <td>Recent advances in large language models (LLMs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chen-etal-2025-instructioncp</td>\n",
       "      <td>2025</td>\n",
       "      <td>{I}nstruction{CP}: A Simple yet Effective Appr...</td>\n",
       "      <td>The rapid development of large language models...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation;Mitigation</td>\n",
       "      <td>safety;multilingual</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mittal-etal-2025-protect</td>\n",
       "      <td>2025</td>\n",
       "      <td>{PROTECT}: Policy-Related Organizational Value...</td>\n",
       "      <td>This paper presents PROTECT, a novel policy-dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training;Evaluation</td>\n",
       "      <td>ethical</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>yao-etal-2024-value</td>\n",
       "      <td>2024</td>\n",
       "      <td>Value {FULCRA}: Mapping Large Language Models ...</td>\n",
       "      <td>Value alignment is crucial for the responsible...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training</td>\n",
       "      <td>value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>jones-etal-2024-multi</td>\n",
       "      <td>2024</td>\n",
       "      <td>A Multi-Aspect Framework for Counter Narrative...</td>\n",
       "      <td>Counter narratives - informed responses to hat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>toxicity</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>zhan-etal-2024-removing</td>\n",
       "      <td>2024</td>\n",
       "      <td>Removing {RLHF} Protections in {GPT}-4 via Fin...</td>\n",
       "      <td>As large language models (LLMs) have increased...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>safety</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>du-etal-2024-zhujiu</td>\n",
       "      <td>2024</td>\n",
       "      <td>{Z}hu{J}iu-Knowledge: A Fairer Platform for Ev...</td>\n",
       "      <td>The swift advancement in large language models...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>General</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>hu-etal-2024-language</td>\n",
       "      <td>2024</td>\n",
       "      <td>Language Models are Alignable Decision-Makers:...</td>\n",
       "      <td>In difficult decision-making scenarios, it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Training</td>\n",
       "      <td>ethical</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ID  year  \\\n",
       "0     kravchenko-etal-2025-ualign  2025   \n",
       "1           liu-etal-2025-smaller  2025   \n",
       "2            yu-etal-2025-diverse  2025   \n",
       "3    chen-etal-2025-instructioncp  2025   \n",
       "4        mittal-etal-2025-protect  2025   \n",
       "..                            ...   ...   \n",
       "286           yao-etal-2024-value  2024   \n",
       "287         jones-etal-2024-multi  2024   \n",
       "288       zhan-etal-2024-removing  2024   \n",
       "289           du-etal-2024-zhujiu  2024   \n",
       "290         hu-etal-2024-language  2024   \n",
       "\n",
       "                                                 title  \\\n",
       "0    {UA}lign: {LLM} Alignment Benchmark for the {U...   \n",
       "1    Smaller Large Language Models Can Do Moral Sel...   \n",
       "2    Diverse {AI} Feedback For Large Language Model...   \n",
       "3    {I}nstruction{CP}: A Simple yet Effective Appr...   \n",
       "4    {PROTECT}: Policy-Related Organizational Value...   \n",
       "..                                                 ...   \n",
       "286  Value {FULCRA}: Mapping Large Language Models ...   \n",
       "287  A Multi-Aspect Framework for Counter Narrative...   \n",
       "288  Removing {RLHF} Protections in {GPT}-4 via Fin...   \n",
       "289  {Z}hu{J}iu-Knowledge: A Fairer Platform for Ev...   \n",
       "290  Language Models are Alignable Decision-Makers:...   \n",
       "\n",
       "                                              abstract  alignment  \\\n",
       "0    This paper introduces UAlign, the comprehensiv...          1   \n",
       "1    Self-correction is one of the most amazing eme...          1   \n",
       "2    Recent advances in large language models (LLMs...          1   \n",
       "3    The rapid development of large language models...          1   \n",
       "4    This paper presents PROTECT, a novel policy-dr...          1   \n",
       "..                                                 ...        ...   \n",
       "286  Value alignment is crucial for the responsible...          1   \n",
       "287  Counter narratives - informed responses to hat...          1   \n",
       "288  As large language models (LLMs) have increased...          1   \n",
       "289  The swift advancement in large language models...          1   \n",
       "290  In difficult decision-making scenarios, it is ...          1   \n",
       "\n",
       "     social aspect modality                            type  \\\n",
       "0                1     Text                      Evaluation   \n",
       "1                1     Text           Evaluation;Mitigation   \n",
       "2                1     Text             Training;Evaluation   \n",
       "3                1     Text  Training;Evaluation;Mitigation   \n",
       "4                1     Text             Training;Evaluation   \n",
       "..             ...      ...                             ...   \n",
       "286              1     Text                        Training   \n",
       "287              1     Text                      Evaluation   \n",
       "288              1     Text                      Evaluation   \n",
       "289              1     Text                      Evaluation   \n",
       "290              1     Text                        Training   \n",
       "\n",
       "                subtopic                                              notes  \n",
       "0                  moral  While \"moral\" and \"ethical\" have slightly diff...  \n",
       "1           moral;safety                                                NaN  \n",
       "2                General                                                NaN  \n",
       "3    safety;multilingual                                                NaN  \n",
       "4                ethical                                                NaN  \n",
       "..                   ...                                                ...  \n",
       "286                value                                                NaN  \n",
       "287             toxicity                                                NaN  \n",
       "288               safety                                                NaN  \n",
       "289              General                                                NaN  \n",
       "290              ethical                                                NaN  \n",
       "\n",
       "[281 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b154a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['moral'], ['moral', 'safety'], ['general'], ['safety', 'multilingual'], ['ethical'], ['ethical'], ['social'], ['legal'], ['multilingual'], ['moral'], ['social'], ['opinions', 'demographics'], ['safety', 'personalization'], ['safety'], ['moral', 'cultural'], ['safety', 'diversity'], ['factuality'], ['cultural', 'multilingual'], ['social'], ['safety', 'toxicity'], ['social'], ['cultural'], ['cultural', 'safety'], ['cultural', 'bias'], ['personalization', 'diversity'], ['factuality'], ['general'], ['cultural'], ['general'], ['general'], ['general'], ['social'], ['cultural'], ['ethical'], ['cultural'], ['personalization'], ['safety'], ['general'], ['safety'], ['general'], ['demographics'], ['demographics'], ['general'], ['general'], ['safety'], ['general'], ['general'], ['personalization'], ['general'], ['safety'], ['general'], ['cultural'], ['moral'], ['cultural'], ['safety'], ['safety'], ['safety'], ['general'], ['general'], ['safety'], ['general'], ['cultural'], ['general'], ['diversity'], ['general'], ['diversity'], ['general'], ['factuality'], ['general'], ['general'], ['general'], ['general'], ['personalization'], ['general'], ['general'], ['general'], ['general'], ['cultural'], ['moral'], ['general'], ['general'], ['general'], ['moral'], ['general'], ['general'], ['personalization'], ['diversity'], ['safety'], ['cultural', 'social', 'bias'], ['moral', 'cultural', 'bias'], ['political', 'bias'], ['safety', 'hate'], ['social'], ['factuality'], ['cultural', 'personalization'], ['factuality'], ['humor'], ['safety', 'factuality'], ['safety'], ['general'], ['safety'], ['general'], ['general'], ['factuality'], ['general'], ['demographics'], ['general'], ['safety'], ['safety'], ['demographics', 'opinions', 'bias'], ['general'], ['safety', 'diversity'], ['social', 'cultural'], ['safety'], ['safety'], ['factuality'], ['general'], ['personalization'], ['demographics', 'safety', 'bias'], ['factuality'], ['safety'], ['safety'], ['moral', 'cultural', 'bias'], ['factuality'], ['safety'], ['demographics', 'social', 'bias'], ['personalization'], ['safety'], ['social', 'opinions'], ['diversity', 'personalization'], ['safety'], ['social', 'ethical', 'personalization'], ['safety'], ['demographics', 'social', 'bias'], ['demographics', 'opinions', 'bias'], ['social', 'political', 'bias'], ['general'], ['multilingual'], ['safety'], ['safety', 'factuality'], ['safety'], ['factuality'], ['personalization'], ['demographics', 'diversity'], ['political', 'bias'], ['demographics', 'bias'], ['general'], ['political', 'factuality', 'bias'], ['toxicity'], ['factuality'], ['cultural', 'bias'], ['factuality'], ['factuality'], ['general'], ['cultural', 'multilingual', 'safety'], ['moral', 'cultural'], ['multilingual'], ['personalization'], ['safety'], ['safety'], ['diversity', 'bias'], ['moral', 'political', 'bias'], ['factuality'], ['moral', 'cultural', 'diversity'], ['moral', 'sexism', 'bias'], ['demographics', 'bias'], ['toxicity'], ['personalization'], ['personalization'], ['general'], ['faithfulness'], ['moral'], ['general'], ['general'], ['value'], ['length', 'general'], ['culture'], ['language'], ['safety'], ['safety'], ['value'], ['safety'], ['demographics'], ['safety'], ['personalization'], ['personalization'], ['language'], ['value'], ['general'], ['general'], ['safety'], ['general'], ['value'], ['value'], ['personalization'], ['personalization'], ['social'], ['personalization'], ['social'], ['safety'], ['cultural'], ['personalization'], ['personalization'], ['cultural'], ['general'], ['personalization'], ['culture'], ['language'], ['culture'], ['general'], ['safety'], ['personalization'], ['cultural'], ['personalization'], ['moral'], ['value'], ['moral'], ['factuality'], ['general'], ['general'], ['general'], ['value', 'safety'], ['general'], ['general'], ['general'], ['value'], ['moral'], ['factuality'], ['safety'], ['general'], ['general'], ['personalization'], ['social', 'demographics'], ['personalization'], ['social'], ['cultural'], ['moral', 'value'], ['general'], ['safety'], ['safety'], ['general'], ['safety'], ['general'], ['value'], ['general'], ['general'], ['value'], ['safety'], ['safety'], ['value', 'safety'], ['safety'], ['personalization'], ['personalization'], ['value'], ['cultural', 'factuality'], ['safety'], ['general'], ['social', 'cultural', 'ethical', 'personalization'], ['toxicity'], ['offensiveness'], ['political'], ['general'], ['general'], ['personalization'], ['moral'], ['cultural'], ['cultural'], ['safety'], ['safety'], ['moral'], ['safety', 'value'], ['toxicity', 'safety'], ['safety'], ['factuality'], ['safety', 'general'], ['language', 'cultural'], ['value'], ['toxicity'], ['safety'], ['general'], ['ethical']]\n",
      "{'offensiveness', 'factuality', 'demographics', 'opinions', 'safety', 'ethical', 'personalization', 'moral', 'diversity', 'culture', 'humor', 'social', 'faithfulness', 'legal', 'value', 'political', 'sexism', 'toxicity', 'length', 'general', 'bias', 'language', 'cultural', 'hate', 'multilingual'}\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "subtpcs = df['subtopic'].apply(lambda x: [i.lower() for i in x.split(';') if x != [] and x != '' and i != [] and i != ''])\n",
    "labels_list = subtpcs.to_list()\n",
    "print(labels_list)\n",
    "\n",
    "types = []\n",
    "for i in subtpcs:\n",
    "    for x in i:\n",
    "        types.append(x.lower())\n",
    "# types = subtpcs.to_list().join.unique\n",
    "types = set(types)\n",
    "print(types)\n",
    "print(len(types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "743a893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 unique types: ['offensiveness', 'factuality', 'demographics', 'opinions', 'safety', 'ethical', 'personalization', 'moral', 'diversity', 'culture', 'humor', 'social', 'faithfulness', 'legal', 'value', 'political', 'sexism', 'toxicity', 'length', 'general', 'bias', 'language', 'cultural', 'hate', 'multilingual']\n",
      "Found 25 unique labels: ['bias', 'cultural', 'culture', 'demographics', 'diversity', 'ethical', 'factuality', 'faithfulness', 'general', 'hate', 'humor', 'language', 'legal', 'length', 'moral', 'multilingual', 'offensiveness', 'opinions', 'personalization', 'political', 'safety', 'sexism', 'social', 'toxicity', 'value']\n",
      "Total items: 281\n",
      "Paper years: ['2022', '2023', '2024', '2025']\n"
     ]
    }
   ],
   "source": [
    "# Extract labels\n",
    "# labels_list = df['subtopic'].tolist()\n",
    "# print(labels_list)\n",
    "\n",
    "# Get unique labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binary = mlb.fit_transform(subtpcs)\n",
    "unique_labels = mlb.classes_\n",
    "n_labels = len(unique_labels)\n",
    "unique_years = sorted(list(set(df['year'].to_list())))\n",
    "\n",
    "print(f\"Found {len(types)} unique types: {list(types)}\")\n",
    "print(f\"Found {n_labels} unique labels: {list(unique_labels)}\")\n",
    "print(f\"Total items: {len(df)}\")\n",
    "print(f\"Paper years: {unique_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37d5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate scatter plot for each year\n",
    "angles = np.linspace(0, 2 * np.pi, n_labels, endpoint=False)\n",
    "cluster_centers = {label: np.array([np.cos(angle), np.sin(angle)]) * 3 \n",
    "                   for label, angle in zip(unique_labels, angles)}\n",
    "\n",
    "# Position each point based on its labels\n",
    "positions = []\n",
    "for lbls in labels_list:\n",
    "    if len(lbls) == 1:\n",
    "        # Single label: near cluster center with small jitter\n",
    "        center = cluster_centers[lbls[0]]\n",
    "        offset = np.random.randn(2) * 0.3\n",
    "        pos = center + offset\n",
    "    else:\n",
    "        # Multiple labels: average of cluster centers\n",
    "        centers = np.array([cluster_centers[lbl] for lbl in lbls if lbl != ''])\n",
    "        if len(centers) > 0:\n",
    "            center = centers.mean(axis=0)\n",
    "            offset = np.random.randn(2) * 0.2\n",
    "            pos = center + offset\n",
    "        else:\n",
    "            pos = np.array([0, 0])  # Default position if all labels are empty\n",
    "    positions.append(pos)\n",
    "\n",
    "positions = np.array(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5203b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles = np.linspace(0, 2 * np.pi, n_labels, endpoint=False)\n",
    "# cluster_centers = {label: np.array([np.cos(angle), np.sin(angle)]) * 3 \n",
    "#                    for label, angle in zip(unique_labels, angles)}\n",
    "\n",
    "# # Initial positioning\n",
    "# positions = []\n",
    "# for lbls in labels_list:\n",
    "#     if len(lbls) == 1:\n",
    "#         center = cluster_centers[lbls[0]]\n",
    "#         offset = np.random.randn(2) * 0.3\n",
    "#         pos = center + offset\n",
    "#     else:\n",
    "#         centers = np.array([cluster_centers[lbl] for lbl in lbls if lbl != ''])\n",
    "#         if len(centers) > 0:\n",
    "#             center = centers.mean(axis=0)\n",
    "#             offset = np.random.randn(2) * 0.2\n",
    "#             pos = center + offset\n",
    "#         else:\n",
    "#             pos = np.array([0, 0])\n",
    "#     positions.append(pos)\n",
    "\n",
    "# positions = np.array(positions)\n",
    "\n",
    "# # Iteratively push overlapping points apart\n",
    "# min_distance = 0.4\n",
    "# n_iterations = 50\n",
    "# repulsion_strength = 0.1\n",
    "\n",
    "# for iteration in range(n_iterations):\n",
    "#     forces = np.zeros_like(positions)\n",
    "    \n",
    "#     for i in range(len(positions)):\n",
    "#         for j in range(i + 1, len(positions)):\n",
    "#             diff = positions[i] - positions[j]\n",
    "#             dist = np.linalg.norm(diff)\n",
    "            \n",
    "#             if dist < min_distance and dist > 0:\n",
    "#                 # Calculate repulsion force\n",
    "#                 force = (diff / dist) * repulsion_strength * (min_distance - dist)\n",
    "#                 forces[i] += force\n",
    "#                 forces[j] -= force\n",
    "    \n",
    "#     # Apply forces\n",
    "#     positions += forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f09f090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = px.colors.qualitative.Set3\n",
    "if len(unique_labels) > len(color_palette):\n",
    "    color_palette = px.colors.sample_colorscale(\"turbo\", [n/(len(unique_labels)-1) for n in range(len(unique_labels))])\n",
    "label_colors = {label: color_palette[i % len(color_palette)] for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "306fcf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2022"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "EU27ud8s+z8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "zXT3fnzIA0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2022",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (4)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "/x4FgrpI9L/uOI6VfQrzv2SwPsMzwvG/OoEpSdLk3L8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "bdIhgk4FBkA3MYhOwSUGQDztOiYh2AlA8Ubxac4ZBUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2022"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "jRsMvXGrBsA=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "3CjHoYKm7r8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2022"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "G1klQviu2r8=",
          "dtype": "f8"
         },
         "xaxis": "x",
         "y": {
          "bdata": "Jn6H+R/MCcA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2023"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "P145c1EYCkA=",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "bvK0vnxj2j8=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2023"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "OeP4qyQ39T8=",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "BBdvQnmUBkA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2023"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5rEVjsrI0z8=",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "avWx4VQBC0A=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2023",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2023",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2023",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2023",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2023",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2023",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2023",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2023",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2023",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2023",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2023"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (11)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "m4IR5lJ/978ilVAPNYv2v6aqSHe+Ofe/m3bahv68678kwQ4tH031v+c1Fx67Wea/LQzGbaV48b/Oq9wuL4nyv8fqcEE4RfK/8HUWEyNx9r9sGmayuVbzvw==",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "JmRy4oHjAUBdp8RVzDsEQE9+OqBoeAVAeyjuuOdRBUB1Z6JqXCsIQCZr5EiLkARAmD7QIKSwB0BqD02pCz8JQP42ja7BYwVAoqlsxP8ACUDaYC162a4BQA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2023"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "ut2M+CMjBcA=",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "N/tJK/of7r8=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2023"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "Tub5Psok478=",
          "dtype": "f8"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "zIKf26suAsA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (18)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "+brSr4UbBUCc+HcdYz3rP1jRZ01sV/o/zh3hm++Q9T/2JChKHtMBQEZrDH1pHfQ/7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/yYKg9OtY/j+3L8sIjwn+P2ZWqDklAQRAILDdcijk9j/fzdKbu+wHQKX+yAYm+AJA4K0b9o/SpD+RcQDvJwTgP9HtmU7rpwVA",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "cLO6mEjm3L9DE4jbyEHLv+R/1ZsJyfm/+S8M262uzb/zaTTuiXXEv4J3oExKArI/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EpvcEYRs979gPcqrLhL0v1S5wZYDUfE/OC8Oju/Yx79XItQBdT7hPy4Hyhxz/PI/qhe7Ls3J8r8UA5eE7Nzwv9gT0Bs4ZfM/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (15)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "hIA1I8nUCUATbQC2TfYFQPHvDI6ZygdA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "DYfPYIMN4D+1JXUurOjqP9HrOliJE9w/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (9)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "/e7RShH9/z9CimAqrJL4P6fL2GGaS+U/XzNx38KHAECl/sgGJvgCQJSbvIBpGuU/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "C+AVVsDKA0Cwf7pCoD2pP8tFgAkMBtK/EPW4m9P+A0AuB8occ/zyP6WbrTlN1PA/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2024"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (2)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "u8Hbly6B6z8D4dge8sPsPw==",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "f7hHgEwF7L8J3yCJ4nkFQA==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2024",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (15)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP+aoOn8iedY/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "iW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQNpmEMkpYwVA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2024",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2024",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2024",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2024",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2024",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2024",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2024",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2024",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2024",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2024",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2024",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2024",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2024",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2024",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2024",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2024",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2024",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2024",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2024"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (23)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "jK/dR4Lv878JfXbHU7Lwv+P4uJjUAPO/LJ6A/crR9r8LW9N0hZfxv7Pzu+2bwvK/orfNLxsg8r8soRfhECf2v8+YbOhVSPe/hMLx+gJD9b9j/7VLZXX0v1CkQh2rQP+/kYQgEf3g87+KjHCZhOP5vwwuhaELpfK//2/KohDk479lsoWbeYD5vxMF9maNBfi/nwvSvC9c+r/qxIilOpTvvwDUFEf4DvS/hBiW3epb1b/WIyubNF/2vw==",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "yAXDowx4CUDLeIV1IlsHQIF4S4bNIgZAFCbHB2tdBUDafEsVI2EFQJEB+fVVNgVA//cqW5auAkA/z1/rd6YFQIR1srx+5gNAYCJcJgmiBUAKp+pYMxADQGu91e2NawZA4M3ovq9kBUDywH/vccsDQJYPQ86quwVAaEZD4YmhBUBJsHO+9BAFQMRTUhwP/AVA8SJc7u2XB0D03X6tymwHQHJ7kdeZ0QVAgh9CU4uXv7+NRYE99yMHQA==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2024"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "zDPXf5hH3z8=",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "9GqpBqZM8z8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2024",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2024"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (9)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "p9p+aR4bB8Cc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D/jaOKzJycHwGzjVjfuGgbA",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "AO6PBqCT8L9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L/c1UuIQl7pv2IqctYZjve/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2024"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2024"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "zh3hm++Q9T+PJVxY14rgP8GZ/d05xfc/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "+S8M262uzb8AUnINnQkFwA69awAz7rq/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2024"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (9)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "isHdv9EY7L+pL58WV0vtP046FQCCmuy/IaAv0pxQ5r+ny9hhmkvlP7vB25cuges/Rwg360aU5L/S1ayWdyTkvzgfK1wzNuu/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "SBi03oq+CcCpPWU6uBnuv5qzIix1vgjAQJjsCfhNB8DLRYAJDAbSv3+4R4BMBey/EGM2ZO4yBcBSuPIFWCYKwJ3LWmjcxgbA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2024"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2024",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2024",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2024",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2024",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2024",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2024",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2024",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2024",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2024",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2024",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2024",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2024",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2024",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2024",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2024",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2024",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2024",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2024"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (36)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dP7LIqbptF/o/2A65493y1T80XT30aQ7/P8PakVCdW/g/2Tj/jDt++j+EGJbd6lvVv2t14QDByuc/",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "BIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwDxAD/4FeAnA1C3XjpJoC8BZtxO+xVX8vyPDwtusKgHA/noXY/rWCMCCH0JTi5e/v5i9mLyABwXA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (8)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "+brSr4UbBUAT1R7Lh7ICQLxPlEHMBQZA7BWtG7G6AkCPJVxY14rgP7vB25cuges/GUn6BQNHA0DJgqD061j+Pw==",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "cLO6mEjm3L8NOPTteaQBwCVq9g2W79i/td0UoNT94D8AUnINnQkFwH+4R4BMBey/+6VUzL18ob8Sm9wRhGz3vw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2024"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (5)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "+MSlcwcNAUBmhIG1ykwFQENa+FvVRAZAw9qRUJ1b+D9mbUKFdxIDQA==",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "zV67J3GV8r8DTayHHfj6v6no0JiYVfi/I8PC26wqAcBiB/BtHzD0vw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2024"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (2)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "NF099GkO/z91lhohzvAHQA==",
          "dtype": "f8"
         },
         "xaxis": "x3",
         "y": {
          "bdata": "WbcTvsVV/L+ZQKwG8zDnvw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "V3EABv8pBEA=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "DgmoLW385D8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (14)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQEWdLgofxwRAS8ovSqlNBUC1hjYl9wgKQEjvG2M1eQVA/u/AsyOk+T83xnKVqFf8Pw==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1P0otcnfXT+8/e1VYJC5a2D8SIUhlG4zqP24FhdsLV+U/egJp+kVE/D/xjVATp9fCvw==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2025",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2025",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2025"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2025",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (5)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (2)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DPw==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvw==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/N8ZylahX/D8=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdA8Y1QE6fXwr8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (5)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5Pw==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8Pw==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2025"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2025",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2025",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2025",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2025",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2025",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2025",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2025",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2025",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2025",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2025",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2025",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2025",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2025",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2025",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2025",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2025",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2025",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2025",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2025",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2025",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2025",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2025",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2025",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2025"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (31)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v3kPN/2rJOi/Gdq8/DTM8b+6L4pL1Gfzv9cDHpUMOQHAiezMvxoa678kSyjrSRz3v17pgHP7jvK/ZHxmntR567/AM9pey8T3v8bEnhigEva/IqkWa3258r/Ype03YOr3vw9UO7H0Ife/gGObLUFd8r+tI7SxhsDmv8qQZ0t3wv2/sX4YuObu87/qenqINrT3vxvuC+tlave/V6xuINDC9788vXkw7HH1v/r9F1RBSO2/0On8aG+R978=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQK2WLz2jdgZAQ+k97NXlB0D/8vG7EuAGQLi2O5IlkPQ/UGE9ZHFpB0D1XXYJQK0CQPAUuz/xuwNAbTz5tgUUBkBLGKzmHnEGQP4/rYGklAhA2hQQEM8DCkC6w8f6NJ8DQIRyvTe7QQVAjDqdKXEEB0D1s4fcLPAGQPyu5zgJygNA9i6vKt+TCEBFrcVCayMEQFXrn6ftDAVArZlsUxeNBUB4Uk0bCHsAQD8L9jMkCAdA27uSzxd5A0A=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2025",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2025",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2025"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrA",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2025"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2025",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2025",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2025",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2025",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2025",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (9)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T8=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "ClRTexBp4b8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2025",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (20)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/qlR8LLytyL+8ZSXNZhDwvwaJc2oVJMy/AxSHpS2M0L9Op4qjnFDfv8/PiIfpBde/nl3lZDHZ4r+/qU2ADPLgv+XQ5C1rs+K/FKGBzvEZ8L+X7fKZcV/iv+QPFvI3Cum/skpBMo+uxr+khnHPexTYv2Atf8zjsOa/NKILXDF94r83xnKVqFf8Pw==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjAPjuJb3rhDMCVAjNpA6AJwBH9HWxUJQfAQCXYenRmCcBsOyBbTMQDwCES3s2DIwnAXEwJCZdJCMDyi3YJ9soFwPK7OUnDqQPAuwbvQTdQCcDv/bbO/jwGwGV/BnWM6QTAiBvXAQMtCMC4qlC1p4gGwNF55GeF1AbAkCYSxkajCcDxjVATp9fCvw==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2025",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2025",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2025",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2025",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2025",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2025",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2025",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2025",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2025",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2025",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2025"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (26)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/GyY4/uzY+j/38eLX65zxPxCSb/NcNeA/+SSI72bg8T80BGdZsj/0P0JcjOGgb/M/hxyU8D8I6T+bCRxjw0H3P24ioIrc4eU/AQ6WCQtU8D+C3lFbx6fgP053uHfDM+w/yfNN6T+f8T+2k0AAflncPzqM0MQ/SgJABX+GD9ih8z/SfB1W0bTyPw==",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrAu/EiA+tDA8AeUkQ5bm0EwO5JsMeG+wPAS99vvfvZBsCPt+Z1QegGwH7voRrDkAjA4TPxzAAnBMAPAdUE8Tj9v6m3L5aqqwjAYIdO3FzfB8Cs85vkPesGwEbBlVuAxAfADh+wYny5BcDQB5pq1hUFwJSdMIhdcPm/m/ZLBlugA8AYoB9T50ENwA==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2025",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2025",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2025",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2025",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (10)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQCAL/a0WOgBACDFJxSc/AUDuP8zA9aAAQD74QKlDsAJAN8ZylahX/D8=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwIYhTGVDNP2/WuoepP+Z+L9rtnrX1fyyv28w9T+h0/y/8Y1QE6fXwr8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "6rsVt2PUAEA=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "xHSCsqbJAsA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2025",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2025",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2025",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2025",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2025",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2025"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (13)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A=",
          "dtype": "f8"
         },
         "xaxis": "x4",
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu778=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Year: 2022",
          "x": 0.10625,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Year: 2023",
          "x": 0.36875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Year: 2024",
          "x": 0.6312500000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Year: 2025",
          "x": 0.89375,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "hovermode": "closest",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "text": "Multi-label Clusters by Year"
        },
        "width": 2800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2125
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.2625,
          0.475
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.525,
          0.7375
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.7875,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x2",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x3",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x4",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create subplots - one for each year\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(unique_years),\n",
    "    subplot_titles=[f'Year: {year}' for year in unique_years],\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "num_indices = 0\n",
    "# Add traces for each year\n",
    "for idx, year in enumerate(unique_years):\n",
    "    col = idx + 1\n",
    "    \n",
    "    # # First, add all points in gray (background)\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=positions[:, 0],\n",
    "    #         y=positions[:, 1],\n",
    "    #         mode='markers',\n",
    "    #         marker=dict(size=8, color='lightgray', opacity=0.3),\n",
    "    #         showlegend=False,\n",
    "    #         hoverinfo='skip',\n",
    "    #         name='Background'\n",
    "    #     ),\n",
    "    #     row=1, col=col\n",
    "    # )\n",
    "    \n",
    "    # Then add each cluster for this year\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, lbls in enumerate(labels_list) if label in lbls and years[i] == year]\n",
    "        num_indices += len(indices)\n",
    "        \n",
    "        if len(indices) > 0:\n",
    "            cluster_points = positions[indices]\n",
    "            \n",
    "            # Create hover text with paper IDs and all labels\n",
    "            hover_texts = []\n",
    "            for i in indices:\n",
    "                paper = \"<br>\".join(wrap(df.iloc[i]['title'], width=50))\n",
    "                all_labels = ', '.join(labels_list[i])\n",
    "                paper_id = df.iloc[i]['ID']\n",
    "                hover_texts.append(f\"Paper: {paper}<br>ID: {paper_id}<br>Subtopics: {all_labels}<br>Year: {year}\")\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=cluster_points[:, 0],\n",
    "                    y=cluster_points[:, 1],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=label_colors[label],\n",
    "                        opacity=0.7,\n",
    "                        line=dict(color='black', width=1)\n",
    "                    ),\n",
    "                    name=f'{label} ({len(indices)})',\n",
    "                    legendgroup=label,\n",
    "                    showlegend=(idx == 0),  # Only show legend for first subplot\n",
    "                    hovertext=hover_texts,\n",
    "                    hoverinfo='text'\n",
    "                ),\n",
    "                row=1, col=col\n",
    "            )\n",
    "    \n",
    "    # Update axes for this subplot\n",
    "    fig.update_xaxes(\n",
    "        range=[-4.5, 4.5],\n",
    "        # title_text='Dimension 1',\n",
    "        title_text='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        row=1, col=col\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        range=[-4.5, 4.5],\n",
    "        # title_text='Dimension 2' if col == 1 else '',\n",
    "        title_text='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        scaleanchor=f\"x{col if col > 1 else ''}\",\n",
    "        scaleratio=1,\n",
    "        row=1, col=col\n",
    "    )\n",
    "\n",
    "# Update overall layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=700 * len(unique_years),\n",
    "    title_text=\"Multi-label Clusters by Year\",\n",
    "    title_font_size=16,\n",
    "    hovermode='closest',\n",
    "    template='plotly_white',\n",
    "    # showlegend=True,\n",
    "    # legend=dict(\n",
    "    #     yanchor=\"top\",\n",
    "    #     y=0.99,\n",
    "    #     xanchor=\"left\",\n",
    "    #     x=1.01\n",
    "    # )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4edc9936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2022",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2022",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2022",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2022",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2022",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2022",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2022",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2022",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2022",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2022",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2022",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2022",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2022",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2022",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2022"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (19)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "V3EABv8pBED5utKvhRsFQJz4dx1jPes/WNFnTWxX+j/OHeGb75D1P/YkKEoe0wFARmsMfWkd9D/sFa0bsboCQBlJ+gUDRwNAwZn93TnF9z/JgqD061j+P7cvywiPCf4/ZlaoOSUBBEAgsN1yKOT2P9/N0pu77AdApf7IBib4AkDgrRv2j9KkP5FxAO8nBOA/0e2ZTuunBUA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DgmoLW385D9ws7qYSObcv0MTiNvIQcu/5H/VmwnJ+b/5Lwzbra7Nv/NpNO6JdcS/gnegTEoCsj+13RSg1P3gP/ulVMy9fKG/Dr1rADPuur8Sm9wRhGz3v2A9yqsuEvS/VLnBlgNR8T84Lw6O79jHv1ci1AF1PuE/LgfKHHP88j+qF7suzcnyvxQDl4Ts3PC/2BPQGzhl8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2022",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2022",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2022",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2022",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2022",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2022",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2022",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2022",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2022",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2022",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2022",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2022",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2022",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2022",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2022",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2022",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2022"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (30)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQISANSPJ1AlAE20Atk32BUDx7wyOmcoHQD9eOXNRGApA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/RZ0uCh/HBEBLyi9KqU0FQLWGNiX3CApASO8bYzV5BUD+78CzI6T5PzfGcpWoV/w/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1Pw2Hz2CDDeA/tSV1Lqzo6j/R6zpYiRPcP27ytL58Y9o/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/Si1yd9dP7z97VVgkLlrYPxIhSGUbjOo/bgWF2wtX5T96Amn6RUT8P/GNUBOn18K/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2022",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2022",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2022"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture (3)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2022",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2022",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2022",
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2022",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2022",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2022",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2022",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2022",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2022",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2022",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2022",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2022"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (14)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2022",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2022",
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2022",
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2022",
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2022",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2022",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2022",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2022",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2022",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2022"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (10)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DP/3u0UoR/f8/OeP4qyQ39T8RTbu53yz7P0KKYCqskvg/p8vYYZpL5T9fM3HfwocAQKX+yAYm+AJAlJu8gGka5T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvwvgFVbAygNABBdvQnmUBkDNdPd+fMgDQLB/ukKgPak/y0WACQwG0r8Q9bib0/4DQC4Hyhxz/PI/pZutOU3U8D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2022",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2022",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2022",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2022",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2022",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2022"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (6)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/u8Hbly6B6z83xnKVqFf8PwPh2B7yw+w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdAf7hHgEwF7L/xjVATp9fCvwnfIInieQVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2022",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2022",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2022",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2022",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2022",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2022",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2022",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2022",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2022",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2022",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2022",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2022",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2022"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (21)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P+axFY7KyNM/UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5P+aoOn8iedY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQGr1seFUAQtAiW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8P9pmEMkpYwVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2022"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2022",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2022",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2022",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2022",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2022",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2022",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2022",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2022",
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2022",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2022",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2022",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2022",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2022",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2022",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2022",
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2022",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2022",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2022",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2022",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2022",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2022",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2022",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2022",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2022",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2022",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2022",
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2022",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2022",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2022",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2022",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2022",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2022",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2022",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2022",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2022",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2022",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2022",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2022",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2022",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2022",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2022",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2022",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2022",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2022",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2022",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2022",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2022",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2022",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2022",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2022",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2022",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2022",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2022",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2022",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2022",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2022",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2022",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2022",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2022",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2022",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2022",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2022"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (69)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v4yv3UeC7/O/CX12x1Oy8L/j+LiY1ADzvyyegP3K0fa/C1vTdIWX8b+z87vtm8Lyv6K3zS8bIPK/LKEX4RAn9r/PmGzoVUj3v5uCEeZSf/e/IpVQDzWL9r+mqkh3vjn3v5t22ob+vOu/JMEOLR9N9b/nNRceu1nmvy0Mxm2lePG/zqvcLi+J8r/H6nBBOEXyv/B1FhMjcfa/bBpmsrlW87//HgWCukj0v+44jpV9CvO/ZLA+wzPC8b86gSlJ0uTcv4TC8foCQ/W/Y/+1S2V19L9QpEIdq0D/v5GEIBH94PO/ioxwmYTj+b8MLoWhC6Xyv/9vyqIQ5OO/ZbKFm3mA+b8TBfZmjQX4v58L0rwvXPq/eQ83/ask6L8Z2rz8NMzxv7ovikvUZ/O/1wMelQw5AcCJ7My/GhrrvyRLKOtJHPe/XumAc/uO8r9kfGae1Hnrv8Az2l7LxPe/xsSeGKAS9r8iqRZrfbnyv9il7Tdg6ve/D1Q7sfQh97+AY5stQV3yv60jtLGGwOa/ypBnS3fC/b+xfhi45u7zv+p6eog2tPe/G+4L62Vq979XrG4g0ML3vzy9eTDscfW/+v0XVEFI7b/Q6fxob5H3v+rEiKU6lO+/ANQUR/gO9L+EGJbd6lvVv9YjK5s0X/a/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQMgFw6MMeAlAy3iFdSJbB0CBeEuGzSIGQBQmxwdrXQVA2nxLFSNhBUCRAfn1VTYFQP/3KluWrgJAP89f63emBUCEdbK8fuYDQCZkcuKB4wFAXafEVcw7BEBPfjqgaHgFQHso7rjnUQVAdWeialwrCEAma+RIi5AEQJg+0CCksAdAag9NqQs/CUD+No2uwWMFQKKpbMT/AAlA2mAtetmuAUBt0iGCTgUGQDcxiE7BJQZAPO06JiHYCUDxRvFpzhkFQGAiXCYJogVACqfqWDMQA0BrvdXtjWsGQODN6L6vZAVA8sB/73HLA0CWD0POqrsFQGhGQ+GJoQVASbBzvvQQBUDEU1IcD/wFQPEiXO7tlwdArZYvPaN2BkBD6T3s1eUHQP/y8bsS4AZAuLY7kiWQ9D9QYT1kcWkHQPVddglArQJA8BS7P/G7A0BtPPm2BRQGQEsYrOYecQZA/j+tgaSUCEDaFBAQzwMKQLrDx/o0nwNAhHK9N7tBBUCMOp0pcQQHQPWzh9ws8AZA/K7nOAnKA0D2Lq8q35MIQEWtxUJrIwRAVeufp+0MBUCtmWxTF40FQHhSTRsIewBAPwv2MyQIB0Dbu5LPF3kDQPTdfq3KbAdAcnuR15nRBUCCH0JTi5e/v41FgT33IwdA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2022"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2022"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2022",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2022",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2022",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2022"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (4)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrAzDPXf5hH3z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/9GqpBqZM8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2022"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2022"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2022",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2022",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2022",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2022",
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2022",
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2022",
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2022",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2022",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2022",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2022",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2022",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2022",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2022",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2022",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2022",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2022",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2022",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2022",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2022"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (20)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj+n2n5pHhsHwLrdjPgjIwXAjRsMvXGrBsCc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/42jisycnB8Bs41Y37hoGwA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8A7o8GoJPwvzf7SSv6H+6/3CjHoYKm7r9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/3NVLiEJe6b9iKnLWGY73vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2022",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2022",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2022",
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2022",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2022",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2022"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (6)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2022"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2022",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2022",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2022",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2022"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (4)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/OHeGb75D1P48lXFjXiuA/wZn93TnF9z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b/5Lwzbra7NvwBScg2dCQXADr1rADPuur8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2022",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2022",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2022",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2022",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2022",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2022",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2022",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2022",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2022",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2022",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2022",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2022"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (31)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/isHdv9EY7L9O5vk+yiTjvxtZJUL4rtq/qS+fFldL7T9OOhUAgprsvyGgL9KcUOa/p8vYYZpL5T+7wduXLoHrP0cIN+tGlOS/0tWslnck5L+qVHwsvK3Iv7xlJc1mEPC/BolzahUkzL8DFIelLYzQv06niqOcUN+/z8+Ih+kF17+eXeVkMdniv7+pTYAM8uC/5dDkLWuz4r8UoYHO8Rnwv5ft8plxX+K/5A8W8jcK6b+ySkEyj67Gv6SGcc97FNi/YC1/zOOw5r80ogtcMX3ivzfGcpWoV/w/OB8rXDM2678=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjASBi03oq+CcDMgp/bqy4CwCZ+h/kfzAnAqT1lOrgZ7r+asyIsdb4IwECY7An4TQfAy0WACQwG0r9/uEeATAXsvxBjNmTuMgXAUrjyBVgmCsA+O4lveuEMwJUCM2kDoAnAEf0dbFQlB8BAJdh6dGYJwGw7IFtMxAPAIRLezYMjCcBcTAkJl0kIwPKLdgn2ygXA8rs5ScOpA8C7Bu9BN1AJwO/9ts7+PAbAZX8GdYzpBMCIG9cBAy0IwLiqULWniAbA0XnkZ4XUBsCQJhLGRqMJwPGNUBOn18K/nctaaNzGBsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2022",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2022",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2022",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2022",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2022",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2022"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political (6)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2022",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2022",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2022",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2022",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2022",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2022",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2022",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2022",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2022",
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2022",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2022",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2022",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2022",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2022",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2022",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2022",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2022",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2022",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2022",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2022",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2022",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2022",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2022",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2022",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2022",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2022",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2022",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2022",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2022",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2022",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2022",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2022",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2022",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2022",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2022",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2022",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2022",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2022",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2022",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2022",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2022",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2022",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2022",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2022",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2022",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2022",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2022",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2022",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2022",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2022",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2022",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2022",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2022",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2022",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2022",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2022",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2022",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2022",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2022",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2022",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2022",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2022"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (62)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dPxsmOP7s2Po/9/Hi1+uc8T8Qkm/zXDXgP/kkiO9m4PE/NARnWbI/9D9CXIzhoG/zP4cclPA/COk/mwkcY8NB9z9uIqCK3OHlPwEOlgkLVPA/gt5RW8en4D9Od7h3wzPsP8nzTek/n/E/tpNAAH5Z3D86jNDEP0oCQAV/hg/YofM/0nwdVtG08j+yyKm6bRf6P9gOuePd8tU/NF099GkO/z/D2pFQnVv4P9k4/4w7fvo/hBiW3epb1b9rdeEAwcrnPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrABIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwLvxIgPrQwPAHlJEOW5tBMDuSbDHhvsDwEvfb7372QbAj7fmdUHoBsB+76Eaw5AIwOEz8cwAJwTADwHVBPE4/b+pty+WqqsIwGCHTtxc3wfArPOb5D3rBsBGwZVbgMQHwA4fsGJ8uQXA0AeaatYVBcCUnTCIXXD5v5v2SwZboAPAGKAfU+dBDcA8QA/+BXgJwNQt146SaAvAWbcTvsVV/L8jw8LbrCoBwP56F2P61gjAgh9CU4uXv7+YvZi8gAcFwA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2022"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism (1)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2022",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2022",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2022",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2022",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2022",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2022",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2022",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2022",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2022",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2022",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2022",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2022",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2022",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2022",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2022",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2022",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2022"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (18)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQPm60q+FGwVAE9Uey4eyAkC8T5RBzAUGQOwVrRuxugJAjyVcWNeK4D+7wduXLoHrPxlJ+gUDRwNAyYKg9OtY/j8gC/2tFjoAQAgxScUnPwFA7j/MwPWgAEA++ECpQ7ACQDfGcpWoV/w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwHCzuphI5ty/DTj07XmkAcAlavYNlu/Yv7XdFKDU/eA/AFJyDZ0JBcB/uEeATAXsv/ulVMy9fKG/EpvcEYRs97+GIUxlQzT9v1rqHqT/mfi/a7Z619X8sr9vMPU/odP8v/GNUBOn18K/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2022",
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2022",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2022",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2022",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2022",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2022"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (6)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "6rsVt2PUAED4xKVzBw0BQGaEgbXKTAVAQ1r4W9VEBkDD2pFQnVv4P2ZtQoV3EgNA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "xHSCsqbJAsDNXrsncZXyvwNNrIcd+Pq/qejQmJhV+L8jw8LbrCoBwGIH8G0fMPS/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2022",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2022",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2022",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2022",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2022",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2022",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2022",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2022",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2022",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2022",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2022",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2022",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2022",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2022",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2022"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (15)",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A0XT30aQ7/P3WWGiHO8AdA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu779ZtxO+xVX8v5lArAbzMOe/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2023",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2023",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2023",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2023",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2023",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2023",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2023",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2023",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2023",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2023",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2023",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2023",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2023",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2023",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2023"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (19)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "V3EABv8pBED5utKvhRsFQJz4dx1jPes/WNFnTWxX+j/OHeGb75D1P/YkKEoe0wFARmsMfWkd9D/sFa0bsboCQBlJ+gUDRwNAwZn93TnF9z/JgqD061j+P7cvywiPCf4/ZlaoOSUBBEAgsN1yKOT2P9/N0pu77AdApf7IBib4AkDgrRv2j9KkP5FxAO8nBOA/0e2ZTuunBUA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DgmoLW385D9ws7qYSObcv0MTiNvIQcu/5H/VmwnJ+b/5Lwzbra7Nv/NpNO6JdcS/gnegTEoCsj+13RSg1P3gP/ulVMy9fKG/Dr1rADPuur8Sm9wRhGz3v2A9yqsuEvS/VLnBlgNR8T84Lw6O79jHv1ci1AF1PuE/LgfKHHP88j+qF7suzcnyvxQDl4Ts3PC/2BPQGzhl8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2023",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2023",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2023",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2023",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2023",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2023",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2023",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2023",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2023",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2023",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2023",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2023",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2023",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2023",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2023",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2023",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2023"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (30)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQISANSPJ1AlAE20Atk32BUDx7wyOmcoHQD9eOXNRGApA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/RZ0uCh/HBEBLyi9KqU0FQLWGNiX3CApASO8bYzV5BUD+78CzI6T5PzfGcpWoV/w/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1Pw2Hz2CDDeA/tSV1Lqzo6j/R6zpYiRPcP27ytL58Y9o/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/Si1yd9dP7z97VVgkLlrYPxIhSGUbjOo/bgWF2wtX5T96Amn6RUT8P/GNUBOn18K/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2023",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2023",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2023"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2023",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2023",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2023",
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2023",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2023",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2023",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2023",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2023",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2023",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2023",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2023",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2023"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (14)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2023",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2023",
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2023",
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2023",
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2023",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2023",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2023",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2023",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2023",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2023"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (10)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DP/3u0UoR/f8/OeP4qyQ39T8RTbu53yz7P0KKYCqskvg/p8vYYZpL5T9fM3HfwocAQKX+yAYm+AJAlJu8gGka5T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvwvgFVbAygNABBdvQnmUBkDNdPd+fMgDQLB/ukKgPak/y0WACQwG0r8Q9bib0/4DQC4Hyhxz/PI/pZutOU3U8D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2023",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2023",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2023",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2023",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2023",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2023"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/u8Hbly6B6z83xnKVqFf8PwPh2B7yw+w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdAf7hHgEwF7L/xjVATp9fCvwnfIInieQVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2023",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2023",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2023",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2023",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2023",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2023",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2023",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2023",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2023",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2023",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2023",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2023",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2023"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (21)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P+axFY7KyNM/UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5P+aoOn8iedY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQGr1seFUAQtAiW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8P9pmEMkpYwVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2023"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2023",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2023",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2023",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2023",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2023",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2023",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2023",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2023",
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2023",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2023",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2023",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2023",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2023",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2023",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2023",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2023",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2023",
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2023",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2023",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2023",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2023",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2023",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2023",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2023",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2023",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2023",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2023",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2023",
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2023",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2023",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2023",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2023",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2023",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2023",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2023",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2023",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2023",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2023",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2023",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2023",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2023",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2023",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2023",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2023",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2023",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2023",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2023",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2023",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2023",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2023",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2023",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2023",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2023",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2023",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2023",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2023",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2023",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2023",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2023",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2023",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2023",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2023",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2023",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2023",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2023",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2023",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2023",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2023",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2023"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (69)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v4yv3UeC7/O/CX12x1Oy8L/j+LiY1ADzvyyegP3K0fa/C1vTdIWX8b+z87vtm8Lyv6K3zS8bIPK/LKEX4RAn9r/PmGzoVUj3v5uCEeZSf/e/IpVQDzWL9r+mqkh3vjn3v5t22ob+vOu/JMEOLR9N9b/nNRceu1nmvy0Mxm2lePG/zqvcLi+J8r/H6nBBOEXyv/B1FhMjcfa/bBpmsrlW87//HgWCukj0v+44jpV9CvO/ZLA+wzPC8b86gSlJ0uTcv4TC8foCQ/W/Y/+1S2V19L9QpEIdq0D/v5GEIBH94PO/ioxwmYTj+b8MLoWhC6Xyv/9vyqIQ5OO/ZbKFm3mA+b8TBfZmjQX4v58L0rwvXPq/eQ83/ask6L8Z2rz8NMzxv7ovikvUZ/O/1wMelQw5AcCJ7My/GhrrvyRLKOtJHPe/XumAc/uO8r9kfGae1Hnrv8Az2l7LxPe/xsSeGKAS9r8iqRZrfbnyv9il7Tdg6ve/D1Q7sfQh97+AY5stQV3yv60jtLGGwOa/ypBnS3fC/b+xfhi45u7zv+p6eog2tPe/G+4L62Vq979XrG4g0ML3vzy9eTDscfW/+v0XVEFI7b/Q6fxob5H3v+rEiKU6lO+/ANQUR/gO9L+EGJbd6lvVv9YjK5s0X/a/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQMgFw6MMeAlAy3iFdSJbB0CBeEuGzSIGQBQmxwdrXQVA2nxLFSNhBUCRAfn1VTYFQP/3KluWrgJAP89f63emBUCEdbK8fuYDQCZkcuKB4wFAXafEVcw7BEBPfjqgaHgFQHso7rjnUQVAdWeialwrCEAma+RIi5AEQJg+0CCksAdAag9NqQs/CUD+No2uwWMFQKKpbMT/AAlA2mAtetmuAUBt0iGCTgUGQDcxiE7BJQZAPO06JiHYCUDxRvFpzhkFQGAiXCYJogVACqfqWDMQA0BrvdXtjWsGQODN6L6vZAVA8sB/73HLA0CWD0POqrsFQGhGQ+GJoQVASbBzvvQQBUDEU1IcD/wFQPEiXO7tlwdArZYvPaN2BkBD6T3s1eUHQP/y8bsS4AZAuLY7kiWQ9D9QYT1kcWkHQPVddglArQJA8BS7P/G7A0BtPPm2BRQGQEsYrOYecQZA/j+tgaSUCEDaFBAQzwMKQLrDx/o0nwNAhHK9N7tBBUCMOp0pcQQHQPWzh9ws8AZA/K7nOAnKA0D2Lq8q35MIQEWtxUJrIwRAVeufp+0MBUCtmWxTF40FQHhSTRsIewBAPwv2MyQIB0Dbu5LPF3kDQPTdfq3KbAdAcnuR15nRBUCCH0JTi5e/v41FgT33IwdA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2023"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2023"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2023",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2023",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2023",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2023"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrAzDPXf5hH3z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/9GqpBqZM8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2023"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2023"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2023",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2023",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2023",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2023",
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2023",
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2023",
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2023",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2023",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2023",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2023",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2023",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2023",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2023",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2023",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2023",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2023",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2023",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2023",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2023"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (20)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj+n2n5pHhsHwLrdjPgjIwXAjRsMvXGrBsCc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/42jisycnB8Bs41Y37hoGwA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8A7o8GoJPwvzf7SSv6H+6/3CjHoYKm7r9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/3NVLiEJe6b9iKnLWGY73vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2023",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2023",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2023",
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2023",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2023",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2023"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2023"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2023",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2023",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2023",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2023"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/OHeGb75D1P48lXFjXiuA/wZn93TnF9z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b/5Lwzbra7NvwBScg2dCQXADr1rADPuur8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2023",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2023",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2023",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2023",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2023",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2023",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2023",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2023",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2023",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2023",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2023",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2023"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (31)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/isHdv9EY7L9O5vk+yiTjvxtZJUL4rtq/qS+fFldL7T9OOhUAgprsvyGgL9KcUOa/p8vYYZpL5T+7wduXLoHrP0cIN+tGlOS/0tWslnck5L+qVHwsvK3Iv7xlJc1mEPC/BolzahUkzL8DFIelLYzQv06niqOcUN+/z8+Ih+kF17+eXeVkMdniv7+pTYAM8uC/5dDkLWuz4r8UoYHO8Rnwv5ft8plxX+K/5A8W8jcK6b+ySkEyj67Gv6SGcc97FNi/YC1/zOOw5r80ogtcMX3ivzfGcpWoV/w/OB8rXDM2678=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjASBi03oq+CcDMgp/bqy4CwCZ+h/kfzAnAqT1lOrgZ7r+asyIsdb4IwECY7An4TQfAy0WACQwG0r9/uEeATAXsvxBjNmTuMgXAUrjyBVgmCsA+O4lveuEMwJUCM2kDoAnAEf0dbFQlB8BAJdh6dGYJwGw7IFtMxAPAIRLezYMjCcBcTAkJl0kIwPKLdgn2ygXA8rs5ScOpA8C7Bu9BN1AJwO/9ts7+PAbAZX8GdYzpBMCIG9cBAy0IwLiqULWniAbA0XnkZ4XUBsCQJhLGRqMJwPGNUBOn18K/nctaaNzGBsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2023",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2023",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2023",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2023",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2023",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2023"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2023",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2023",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2023",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2023",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2023",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2023",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2023",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2023",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2023",
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2023",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2023",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2023",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2023",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2023",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2023",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2023",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2023",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2023",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2023",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2023",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2023",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2023",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2023",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2023",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2023",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2023",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2023",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2023",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2023",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2023",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2023",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2023",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2023",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2023",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2023",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2023",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2023",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2023",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2023",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2023",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2023",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2023",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2023",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2023",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2023",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2023",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2023",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2023",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2023",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2023",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2023",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2023",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2023",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2023",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2023",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2023",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2023",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2023",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2023",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2023",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2023",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2023"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (62)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dPxsmOP7s2Po/9/Hi1+uc8T8Qkm/zXDXgP/kkiO9m4PE/NARnWbI/9D9CXIzhoG/zP4cclPA/COk/mwkcY8NB9z9uIqCK3OHlPwEOlgkLVPA/gt5RW8en4D9Od7h3wzPsP8nzTek/n/E/tpNAAH5Z3D86jNDEP0oCQAV/hg/YofM/0nwdVtG08j+yyKm6bRf6P9gOuePd8tU/NF099GkO/z/D2pFQnVv4P9k4/4w7fvo/hBiW3epb1b9rdeEAwcrnPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrABIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwLvxIgPrQwPAHlJEOW5tBMDuSbDHhvsDwEvfb7372QbAj7fmdUHoBsB+76Eaw5AIwOEz8cwAJwTADwHVBPE4/b+pty+WqqsIwGCHTtxc3wfArPOb5D3rBsBGwZVbgMQHwA4fsGJ8uQXA0AeaatYVBcCUnTCIXXD5v5v2SwZboAPAGKAfU+dBDcA8QA/+BXgJwNQt146SaAvAWbcTvsVV/L8jw8LbrCoBwP56F2P61gjAgh9CU4uXv7+YvZi8gAcFwA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2023"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2023",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2023",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2023",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2023",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2023",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2023",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2023",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2023",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2023",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2023",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2023",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2023",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2023",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2023",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2023",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2023",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2023"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (18)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQPm60q+FGwVAE9Uey4eyAkC8T5RBzAUGQOwVrRuxugJAjyVcWNeK4D+7wduXLoHrPxlJ+gUDRwNAyYKg9OtY/j8gC/2tFjoAQAgxScUnPwFA7j/MwPWgAEA++ECpQ7ACQDfGcpWoV/w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwHCzuphI5ty/DTj07XmkAcAlavYNlu/Yv7XdFKDU/eA/AFJyDZ0JBcB/uEeATAXsv/ulVMy9fKG/EpvcEYRs97+GIUxlQzT9v1rqHqT/mfi/a7Z619X8sr9vMPU/odP8v/GNUBOn18K/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2023",
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2023",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2023",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2023",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2023",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2023"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "6rsVt2PUAED4xKVzBw0BQGaEgbXKTAVAQ1r4W9VEBkDD2pFQnVv4P2ZtQoV3EgNA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "xHSCsqbJAsDNXrsncZXyvwNNrIcd+Pq/qejQmJhV+L8jw8LbrCoBwGIH8G0fMPS/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2023",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2023",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2023",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2023",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2023",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2023",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2023",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2023",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2023",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2023",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2023",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2023",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2023",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2023",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2023"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (15)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A0XT30aQ7/P3WWGiHO8AdA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu779ZtxO+xVX8v5lArAbzMOe/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (19)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "V3EABv8pBED5utKvhRsFQJz4dx1jPes/WNFnTWxX+j/OHeGb75D1P/YkKEoe0wFARmsMfWkd9D/sFa0bsboCQBlJ+gUDRwNAwZn93TnF9z/JgqD061j+P7cvywiPCf4/ZlaoOSUBBEAgsN1yKOT2P9/N0pu77AdApf7IBib4AkDgrRv2j9KkP5FxAO8nBOA/0e2ZTuunBUA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DgmoLW385D9ws7qYSObcv0MTiNvIQcu/5H/VmwnJ+b/5Lwzbra7Nv/NpNO6JdcS/gnegTEoCsj+13RSg1P3gP/ulVMy9fKG/Dr1rADPuur8Sm9wRhGz3v2A9yqsuEvS/VLnBlgNR8T84Lw6O79jHv1ci1AF1PuE/LgfKHHP88j+qF7suzcnyvxQDl4Ts3PC/2BPQGzhl8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2024",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2024",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2024",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2024",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2024",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (30)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQISANSPJ1AlAE20Atk32BUDx7wyOmcoHQD9eOXNRGApA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/RZ0uCh/HBEBLyi9KqU0FQLWGNiX3CApASO8bYzV5BUD+78CzI6T5PzfGcpWoV/w/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1Pw2Hz2CDDeA/tSV1Lqzo6j/R6zpYiRPcP27ytL58Y9o/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/Si1yd9dP7z97VVgkLlrYPxIhSGUbjOo/bgWF2wtX5T96Amn6RUT8P/GNUBOn18K/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2024",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2024",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2024"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2024",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2024"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (14)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2024",
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2024",
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2024",
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (10)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DP/3u0UoR/f8/OeP4qyQ39T8RTbu53yz7P0KKYCqskvg/p8vYYZpL5T9fM3HfwocAQKX+yAYm+AJAlJu8gGka5T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvwvgFVbAygNABBdvQnmUBkDNdPd+fMgDQLB/ukKgPak/y0WACQwG0r8Q9bib0/4DQC4Hyhxz/PI/pZutOU3U8D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2024",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2024",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2024",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2024"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/u8Hbly6B6z83xnKVqFf8PwPh2B7yw+w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdAf7hHgEwF7L/xjVATp9fCvwnfIInieQVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2024",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2024",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2024",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (21)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P+axFY7KyNM/UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5P+aoOn8iedY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQGr1seFUAQtAiW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8P9pmEMkpYwVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2024"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2024",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2024",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2024",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2024",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2024",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2024",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2024",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2024",
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2024",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2024",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2024",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2024",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2024",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2024",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2024",
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2024",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2024",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2024",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2024",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2024",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2024",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2024",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2024",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2024",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2024",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2024",
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2024",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2024",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2024",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2024",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2024",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2024",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2024",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2024",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2024",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2024",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2024",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2024",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2024",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2024",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2024",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2024",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2024",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2024",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2024",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2024",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2024",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2024",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2024",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2024",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2024",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2024",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2024",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2024",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2024",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2024",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2024",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2024",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2024",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2024"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (69)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v4yv3UeC7/O/CX12x1Oy8L/j+LiY1ADzvyyegP3K0fa/C1vTdIWX8b+z87vtm8Lyv6K3zS8bIPK/LKEX4RAn9r/PmGzoVUj3v5uCEeZSf/e/IpVQDzWL9r+mqkh3vjn3v5t22ob+vOu/JMEOLR9N9b/nNRceu1nmvy0Mxm2lePG/zqvcLi+J8r/H6nBBOEXyv/B1FhMjcfa/bBpmsrlW87//HgWCukj0v+44jpV9CvO/ZLA+wzPC8b86gSlJ0uTcv4TC8foCQ/W/Y/+1S2V19L9QpEIdq0D/v5GEIBH94PO/ioxwmYTj+b8MLoWhC6Xyv/9vyqIQ5OO/ZbKFm3mA+b8TBfZmjQX4v58L0rwvXPq/eQ83/ask6L8Z2rz8NMzxv7ovikvUZ/O/1wMelQw5AcCJ7My/GhrrvyRLKOtJHPe/XumAc/uO8r9kfGae1Hnrv8Az2l7LxPe/xsSeGKAS9r8iqRZrfbnyv9il7Tdg6ve/D1Q7sfQh97+AY5stQV3yv60jtLGGwOa/ypBnS3fC/b+xfhi45u7zv+p6eog2tPe/G+4L62Vq979XrG4g0ML3vzy9eTDscfW/+v0XVEFI7b/Q6fxob5H3v+rEiKU6lO+/ANQUR/gO9L+EGJbd6lvVv9YjK5s0X/a/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQMgFw6MMeAlAy3iFdSJbB0CBeEuGzSIGQBQmxwdrXQVA2nxLFSNhBUCRAfn1VTYFQP/3KluWrgJAP89f63emBUCEdbK8fuYDQCZkcuKB4wFAXafEVcw7BEBPfjqgaHgFQHso7rjnUQVAdWeialwrCEAma+RIi5AEQJg+0CCksAdAag9NqQs/CUD+No2uwWMFQKKpbMT/AAlA2mAtetmuAUBt0iGCTgUGQDcxiE7BJQZAPO06JiHYCUDxRvFpzhkFQGAiXCYJogVACqfqWDMQA0BrvdXtjWsGQODN6L6vZAVA8sB/73HLA0CWD0POqrsFQGhGQ+GJoQVASbBzvvQQBUDEU1IcD/wFQPEiXO7tlwdArZYvPaN2BkBD6T3s1eUHQP/y8bsS4AZAuLY7kiWQ9D9QYT1kcWkHQPVddglArQJA8BS7P/G7A0BtPPm2BRQGQEsYrOYecQZA/j+tgaSUCEDaFBAQzwMKQLrDx/o0nwNAhHK9N7tBBUCMOp0pcQQHQPWzh9ws8AZA/K7nOAnKA0D2Lq8q35MIQEWtxUJrIwRAVeufp+0MBUCtmWxTF40FQHhSTRsIewBAPwv2MyQIB0Dbu5LPF3kDQPTdfq3KbAdAcnuR15nRBUCCH0JTi5e/v41FgT33IwdA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2024"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2024",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2024",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2024",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrAzDPXf5hH3z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/9GqpBqZM8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2024"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2024"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2024",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2024",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2024",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2024",
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2024",
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2024",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2024",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2024",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2024",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2024",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2024",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2024"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (20)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj+n2n5pHhsHwLrdjPgjIwXAjRsMvXGrBsCc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/42jisycnB8Bs41Y37hoGwA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8A7o8GoJPwvzf7SSv6H+6/3CjHoYKm7r9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/3NVLiEJe6b9iKnLWGY73vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2024",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2024",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2024",
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2024"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2024"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/OHeGb75D1P48lXFjXiuA/wZn93TnF9z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b/5Lwzbra7NvwBScg2dCQXADr1rADPuur8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2024",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2024",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2024",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2024",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2024",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2024"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (31)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/isHdv9EY7L9O5vk+yiTjvxtZJUL4rtq/qS+fFldL7T9OOhUAgprsvyGgL9KcUOa/p8vYYZpL5T+7wduXLoHrP0cIN+tGlOS/0tWslnck5L+qVHwsvK3Iv7xlJc1mEPC/BolzahUkzL8DFIelLYzQv06niqOcUN+/z8+Ih+kF17+eXeVkMdniv7+pTYAM8uC/5dDkLWuz4r8UoYHO8Rnwv5ft8plxX+K/5A8W8jcK6b+ySkEyj67Gv6SGcc97FNi/YC1/zOOw5r80ogtcMX3ivzfGcpWoV/w/OB8rXDM2678=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjASBi03oq+CcDMgp/bqy4CwCZ+h/kfzAnAqT1lOrgZ7r+asyIsdb4IwECY7An4TQfAy0WACQwG0r9/uEeATAXsvxBjNmTuMgXAUrjyBVgmCsA+O4lveuEMwJUCM2kDoAnAEf0dbFQlB8BAJdh6dGYJwGw7IFtMxAPAIRLezYMjCcBcTAkJl0kIwPKLdgn2ygXA8rs5ScOpA8C7Bu9BN1AJwO/9ts7+PAbAZX8GdYzpBMCIG9cBAy0IwLiqULWniAbA0XnkZ4XUBsCQJhLGRqMJwPGNUBOn18K/nctaaNzGBsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2024"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2024",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2024",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2024",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2024",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2024",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2024",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2024",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2024",
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2024",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2024",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2024",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2024",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2024",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2024",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2024",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2024",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2024",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2024",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2024",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2024",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2024",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2024",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2024",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2024",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2024",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2024",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2024",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2024",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2024",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2024",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2024",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2024",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2024",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2024",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2024",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2024",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2024",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2024",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2024",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2024",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2024",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2024"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (62)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dPxsmOP7s2Po/9/Hi1+uc8T8Qkm/zXDXgP/kkiO9m4PE/NARnWbI/9D9CXIzhoG/zP4cclPA/COk/mwkcY8NB9z9uIqCK3OHlPwEOlgkLVPA/gt5RW8en4D9Od7h3wzPsP8nzTek/n/E/tpNAAH5Z3D86jNDEP0oCQAV/hg/YofM/0nwdVtG08j+yyKm6bRf6P9gOuePd8tU/NF099GkO/z/D2pFQnVv4P9k4/4w7fvo/hBiW3epb1b9rdeEAwcrnPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrABIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwLvxIgPrQwPAHlJEOW5tBMDuSbDHhvsDwEvfb7372QbAj7fmdUHoBsB+76Eaw5AIwOEz8cwAJwTADwHVBPE4/b+pty+WqqsIwGCHTtxc3wfArPOb5D3rBsBGwZVbgMQHwA4fsGJ8uQXA0AeaatYVBcCUnTCIXXD5v5v2SwZboAPAGKAfU+dBDcA8QA/+BXgJwNQt146SaAvAWbcTvsVV/L8jw8LbrCoBwP56F2P61gjAgh9CU4uXv7+YvZi8gAcFwA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2024",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2024",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2024",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2024",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2024",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2024",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2024",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2024",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2024",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2024"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (18)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQPm60q+FGwVAE9Uey4eyAkC8T5RBzAUGQOwVrRuxugJAjyVcWNeK4D+7wduXLoHrPxlJ+gUDRwNAyYKg9OtY/j8gC/2tFjoAQAgxScUnPwFA7j/MwPWgAEA++ECpQ7ACQDfGcpWoV/w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwHCzuphI5ty/DTj07XmkAcAlavYNlu/Yv7XdFKDU/eA/AFJyDZ0JBcB/uEeATAXsv/ulVMy9fKG/EpvcEYRs97+GIUxlQzT9v1rqHqT/mfi/a7Z619X8sr9vMPU/odP8v/GNUBOn18K/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2024",
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2024"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "6rsVt2PUAED4xKVzBw0BQGaEgbXKTAVAQ1r4W9VEBkDD2pFQnVv4P2ZtQoV3EgNA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "xHSCsqbJAsDNXrsncZXyvwNNrIcd+Pq/qejQmJhV+L8jw8LbrCoBwGIH8G0fMPS/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2024",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2024",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2024",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2024",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2024",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2024",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2024",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2024",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2024",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2024",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2024",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2024",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2024",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2024"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (15)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A0XT30aQ7/P3WWGiHO8AdA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu779ZtxO+xVX8v5lArAbzMOe/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2025",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2025",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2025",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2025",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2025",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2025",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2025",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2025",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2025",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2025",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2025",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2025",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2025"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias (19)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "V3EABv8pBED5utKvhRsFQJz4dx1jPes/WNFnTWxX+j/OHeGb75D1P/YkKEoe0wFARmsMfWkd9D/sFa0bsboCQBlJ+gUDRwNAwZn93TnF9z/JgqD061j+P7cvywiPCf4/ZlaoOSUBBEAgsN1yKOT2P9/N0pu77AdApf7IBib4AkDgrRv2j9KkP5FxAO8nBOA/0e2ZTuunBUA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DgmoLW385D9ws7qYSObcv0MTiNvIQcu/5H/VmwnJ+b/5Lwzbra7Nv/NpNO6JdcS/gnegTEoCsj+13RSg1P3gP/ulVMy9fKG/Dr1rADPuur8Sm9wRhGz3v2A9yqsuEvS/VLnBlgNR8T84Lw6O79jHv1ci1AF1PuE/LgfKHHP88j+qF7suzcnyvxQDl4Ts3PC/2BPQGzhl8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2025",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2025",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2025",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2025",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2025",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2025"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural (30)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQISANSPJ1AlAE20Atk32BUDx7wyOmcoHQD9eOXNRGApA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/RZ0uCh/HBEBLyi9KqU0FQLWGNiX3CApASO8bYzV5BUD+78CzI6T5PzfGcpWoV/w/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1Pw2Hz2CDDeA/tSV1Lqzo6j/R6zpYiRPcP27ytL58Y9o/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/Si1yd9dP7z97VVgkLlrYPxIhSGUbjOo/bgWF2wtX5T96Amn6RUT8P/GNUBOn18K/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2025",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2025",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2025"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture (3)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2025",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2025",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2025",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2025",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2025",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2025",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics (14)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025",
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2025",
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2025",
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2025",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2025",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2025",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2025",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2025"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity (10)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DP/3u0UoR/f8/OeP4qyQ39T8RTbu53yz7P0KKYCqskvg/p8vYYZpL5T9fM3HfwocAQKX+yAYm+AJAlJu8gGka5T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvwvgFVbAygNABBdvQnmUBkDNdPd+fMgDQLB/ukKgPak/y0WACQwG0r8Q9bib0/4DQC4Hyhxz/PI/pZutOU3U8D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2025",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2025"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/u8Hbly6B6z83xnKVqFf8PwPh2B7yw+w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdAf7hHgEwF7L/xjVATp9fCvwnfIInieQVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2025",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2025",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2025",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2025",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2025",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2025"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality (21)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P+axFY7KyNM/UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5P+aoOn8iedY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQGr1seFUAQtAiW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8P9pmEMkpYwVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2025"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2025",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2025",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2025",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2025",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2025",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2025",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2025",
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2025",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2025",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2025",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2025",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2025",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2025",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2025",
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2025",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2025",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2025",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2025",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2025",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2025",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2025",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2025",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2025",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2025",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2025",
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2025",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2025",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2025",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2025",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2025",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2025",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2025",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2025",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2025",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2025",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2025",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2025",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2025",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2025",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2025",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2025",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2025",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2025",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2025",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2025",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2025",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2025",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2025",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2025",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2025",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2025",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2025",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2025",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2025",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2025",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2025",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2025"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general (69)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v4yv3UeC7/O/CX12x1Oy8L/j+LiY1ADzvyyegP3K0fa/C1vTdIWX8b+z87vtm8Lyv6K3zS8bIPK/LKEX4RAn9r/PmGzoVUj3v5uCEeZSf/e/IpVQDzWL9r+mqkh3vjn3v5t22ob+vOu/JMEOLR9N9b/nNRceu1nmvy0Mxm2lePG/zqvcLi+J8r/H6nBBOEXyv/B1FhMjcfa/bBpmsrlW87//HgWCukj0v+44jpV9CvO/ZLA+wzPC8b86gSlJ0uTcv4TC8foCQ/W/Y/+1S2V19L9QpEIdq0D/v5GEIBH94PO/ioxwmYTj+b8MLoWhC6Xyv/9vyqIQ5OO/ZbKFm3mA+b8TBfZmjQX4v58L0rwvXPq/eQ83/ask6L8Z2rz8NMzxv7ovikvUZ/O/1wMelQw5AcCJ7My/GhrrvyRLKOtJHPe/XumAc/uO8r9kfGae1Hnrv8Az2l7LxPe/xsSeGKAS9r8iqRZrfbnyv9il7Tdg6ve/D1Q7sfQh97+AY5stQV3yv60jtLGGwOa/ypBnS3fC/b+xfhi45u7zv+p6eog2tPe/G+4L62Vq979XrG4g0ML3vzy9eTDscfW/+v0XVEFI7b/Q6fxob5H3v+rEiKU6lO+/ANQUR/gO9L+EGJbd6lvVv9YjK5s0X/a/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQMgFw6MMeAlAy3iFdSJbB0CBeEuGzSIGQBQmxwdrXQVA2nxLFSNhBUCRAfn1VTYFQP/3KluWrgJAP89f63emBUCEdbK8fuYDQCZkcuKB4wFAXafEVcw7BEBPfjqgaHgFQHso7rjnUQVAdWeialwrCEAma+RIi5AEQJg+0CCksAdAag9NqQs/CUD+No2uwWMFQKKpbMT/AAlA2mAtetmuAUBt0iGCTgUGQDcxiE7BJQZAPO06JiHYCUDxRvFpzhkFQGAiXCYJogVACqfqWDMQA0BrvdXtjWsGQODN6L6vZAVA8sB/73HLA0CWD0POqrsFQGhGQ+GJoQVASbBzvvQQBUDEU1IcD/wFQPEiXO7tlwdArZYvPaN2BkBD6T3s1eUHQP/y8bsS4AZAuLY7kiWQ9D9QYT1kcWkHQPVddglArQJA8BS7P/G7A0BtPPm2BRQGQEsYrOYecQZA/j+tgaSUCEDaFBAQzwMKQLrDx/o0nwNAhHK9N7tBBUCMOp0pcQQHQPWzh9ws8AZA/K7nOAnKA0D2Lq8q35MIQEWtxUJrIwRAVeufp+0MBUCtmWxTF40FQHhSTRsIewBAPwv2MyQIB0Dbu5LPF3kDQPTdfq3KbAdAcnuR15nRBUCCH0JTi5e/v41FgT33IwdA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2025"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2025"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2025",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2025",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2025",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2025"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrAzDPXf5hH3z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/9GqpBqZM8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2025"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2025",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2025",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2025",
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2025",
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2025",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2025",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2025",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2025",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2025",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2025",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2025",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2025",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2025",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2025"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral (20)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj+n2n5pHhsHwLrdjPgjIwXAjRsMvXGrBsCc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/42jisycnB8Bs41Y37hoGwA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8A7o8GoJPwvzf7SSv6H+6/3CjHoYKm7r9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/3NVLiEJe6b9iKnLWGY73vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025",
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2025",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2025",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2025"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2025"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2025",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2025",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2025"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions (4)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "DMeILJpm0T/OHeGb75D1P48lXFjXiuA/wZn93TnF9z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b/5Lwzbra7NvwBScg2dCQXADr1rADPuur8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2025",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2025",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2025",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2025",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2025",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2025"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization (31)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/isHdv9EY7L9O5vk+yiTjvxtZJUL4rtq/qS+fFldL7T9OOhUAgprsvyGgL9KcUOa/p8vYYZpL5T+7wduXLoHrP0cIN+tGlOS/0tWslnck5L+qVHwsvK3Iv7xlJc1mEPC/BolzahUkzL8DFIelLYzQv06niqOcUN+/z8+Ih+kF17+eXeVkMdniv7+pTYAM8uC/5dDkLWuz4r8UoYHO8Rnwv5ft8plxX+K/5A8W8jcK6b+ySkEyj67Gv6SGcc97FNi/YC1/zOOw5r80ogtcMX3ivzfGcpWoV/w/OB8rXDM2678=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjASBi03oq+CcDMgp/bqy4CwCZ+h/kfzAnAqT1lOrgZ7r+asyIsdb4IwECY7An4TQfAy0WACQwG0r9/uEeATAXsvxBjNmTuMgXAUrjyBVgmCsA+O4lveuEMwJUCM2kDoAnAEf0dbFQlB8BAJdh6dGYJwGw7IFtMxAPAIRLezYMjCcBcTAkJl0kIwPKLdgn2ygXA8rs5ScOpA8C7Bu9BN1AJwO/9ts7+PAbAZX8GdYzpBMCIG9cBAy0IwLiqULWniAbA0XnkZ4XUBsCQJhLGRqMJwPGNUBOn18K/nctaaNzGBsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2025",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2025",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2025",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2025",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2025",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2025"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2025",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2025",
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2025",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2025",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2025",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2025",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2025",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2025",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2025",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2025",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2025",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2025",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2025",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2025",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2025",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2025",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2025",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2025",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2025",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2025",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2025",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2025",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2025",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2025",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2025",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2025",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2025",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2025",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2025",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2025",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2025",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2025",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2025",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2025",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2025",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2025",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2025",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2025",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2025",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2025",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2025",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2025",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2025",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2025",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2025",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2025"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety (62)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dPxsmOP7s2Po/9/Hi1+uc8T8Qkm/zXDXgP/kkiO9m4PE/NARnWbI/9D9CXIzhoG/zP4cclPA/COk/mwkcY8NB9z9uIqCK3OHlPwEOlgkLVPA/gt5RW8en4D9Od7h3wzPsP8nzTek/n/E/tpNAAH5Z3D86jNDEP0oCQAV/hg/YofM/0nwdVtG08j+yyKm6bRf6P9gOuePd8tU/NF099GkO/z/D2pFQnVv4P9k4/4w7fvo/hBiW3epb1b9rdeEAwcrnPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrABIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwLvxIgPrQwPAHlJEOW5tBMDuSbDHhvsDwEvfb7372QbAj7fmdUHoBsB+76Eaw5AIwOEz8cwAJwTADwHVBPE4/b+pty+WqqsIwGCHTtxc3wfArPOb5D3rBsBGwZVbgMQHwA4fsGJ8uQXA0AeaatYVBcCUnTCIXXD5v5v2SwZboAPAGKAfU+dBDcA8QA/+BXgJwNQt146SaAvAWbcTvsVV/L8jw8LbrCoBwP56F2P61gjAgh9CU4uXv7+YvZi8gAcFwA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2025"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism (1)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2025",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2025",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2025",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2025",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2025",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2025",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2025",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2025",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2025",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2025",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2025",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social (18)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQPm60q+FGwVAE9Uey4eyAkC8T5RBzAUGQOwVrRuxugJAjyVcWNeK4D+7wduXLoHrPxlJ+gUDRwNAyYKg9OtY/j8gC/2tFjoAQAgxScUnPwFA7j/MwPWgAEA++ECpQ7ACQDfGcpWoV/w/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwHCzuphI5ty/DTj07XmkAcAlavYNlu/Yv7XdFKDU/eA/AFJyDZ0JBcB/uEeATAXsv/ulVMy9fKG/EpvcEYRs97+GIUxlQzT9v1rqHqT/mfi/a7Z619X8sr9vMPU/odP8v/GNUBOn18K/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025",
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2025",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2025",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2025",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2025",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2025"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity (6)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "6rsVt2PUAED4xKVzBw0BQGaEgbXKTAVAQ1r4W9VEBkDD2pFQnVv4P2ZtQoV3EgNA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "xHSCsqbJAsDNXrsncZXyvwNNrIcd+Pq/qejQmJhV+L8jw8LbrCoBwGIH8G0fMPS/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2025",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2025",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2025",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2025",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2025",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2025",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2025",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2025"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value (15)",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A0XT30aQ7/P3WWGiHO8AdA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu779ZtxO+xVX8v5lArAbzMOe/",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "height": 800,
        "hovermode": "closest",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "text": "Multi-label Clusters (All Years)"
        },
        "width": 1000,
        "xaxis": {
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hover_texts = []\n",
    "indices = [i for i, lbls in enumerate(labels_list) if label in lbls]\n",
    "for i in indices:\n",
    "    paper = \"<br>\".join(wrap(df.iloc[i]['title'], width=50))\n",
    "    all_labels = ', '.join(labels_list[i])\n",
    "    paper_id = df.iloc[i]['ID']\n",
    "    hover_texts.append(f\"Paper: {paper}<br>ID: {paper_id}<br>Subtopics: {all_labels}<br>Year: {year}\")\n",
    "\n",
    "cluster_points = positions[indices]\n",
    "\n",
    "all_fig = go.Figure()\n",
    "\n",
    "for idx, year in enumerate(unique_years):\n",
    "    col = idx + 1\n",
    "    \n",
    "    # Then add each cluster for this year\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, lbls in enumerate(labels_list) if label in lbls]\n",
    "        num_indices += len(indices)\n",
    "        \n",
    "        if len(indices) > 0:\n",
    "            cluster_points = positions[indices]\n",
    "            \n",
    "            # Create hover text with paper IDs and all labels\n",
    "            hover_texts = []\n",
    "            for i in indices:\n",
    "                paper = \"<br>\".join(wrap(df.iloc[i]['title'], width=50))\n",
    "                all_labels = ', '.join(labels_list[i])\n",
    "                paper_id = df.iloc[i]['ID']\n",
    "                hover_texts.append(f\"Paper: {paper}<br>ID: {paper_id}<br>Subtopics: {all_labels}<br>Year: {year}\")\n",
    "            \n",
    "            all_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=cluster_points[:, 0],\n",
    "                    y=cluster_points[:, 1],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=label_colors[label],\n",
    "                        opacity=0.7,\n",
    "                        line=dict(color='black', width=1)\n",
    "                    ),\n",
    "                    name=f'{label} ({len(indices)})',\n",
    "                    legendgroup=label,\n",
    "                    showlegend=(idx == 0),  # Only show legend for first subplot\n",
    "                    hovertext=hover_texts,\n",
    "                    hoverinfo='text'\n",
    "                )\n",
    "            )\n",
    "    \n",
    "# Update axes for this subplot\n",
    "all_fig.update_layout(\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title_text=\"Multi-label Clusters (All Years)\",\n",
    "    title_font_size=16,\n",
    "    hovermode='closest',\n",
    "    template='plotly_white',\n",
    "    xaxis=dict(\n",
    "        range=[-4.5, 4.5],\n",
    "        title='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[-4.5, 4.5],\n",
    "        title='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1\n",
    "    )\n",
    ")\n",
    "\n",
    "all_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db7651ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "bias",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "cultural",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "culture",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "demographics",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "diversity",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "ethical",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "factuality",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "faithfulness",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "general",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "hate",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "humor",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "language",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "legal",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "length",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "moral",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "multilingual",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "offensiveness",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "opinions",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "personalization",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "political",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "safety",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "sexism",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "social",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "toxicity",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 10
         },
         "mode": "markers",
         "name": "value",
         "showlegend": true,
         "type": "scatter",
         "visible": true,
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: The (Undesired) Attenuation of Human Biases by<br>Multilinguality<br>ID: espana-bonet-barron-cedeno-2022-undesired<br>Subtopics: diversity<br>Year: 2022"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity",
         "showlegend": false,
         "type": "scatter",
         "visible": true,
         "x": {
          "bdata": "EU27ud8s+z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "zXT3fnzIA0A=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Does Moral Code have a Moral Code? Probing<br>Delphi{'}s Moral Philosophy<br>ID: fraser-etal-2022-moral<br>Subtopics: general<br>Year: 2022",
          "Paper: Towards Socially Intelligent Agents with Mental<br>State Transition and Human Value<br>ID: qiu-etal-2022-towards<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning to Social Norms and Values in Interactive<br>Narratives<br>ID: ammanabrolu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022",
          "Paper: Aligning Generative Language Models with Human<br>Values<br>ID: liu-etal-2022-aligning<br>Subtopics: general<br>Year: 2022"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general",
         "showlegend": false,
         "type": "scatter",
         "visible": true,
         "x": {
          "bdata": "/x4FgrpI9L/uOI6VfQrzv2SwPsMzwvG/OoEpSdLk3L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "bdIhgk4FBkA3MYhOwSUGQDztOiYh2AlA8Ubxac4ZBUA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: A Corpus for Understanding and Generating Moral<br>Stories<br>ID: guan-etal-2022-corpus<br>Subtopics: moral<br>Year: 2022"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral",
         "showlegend": false,
         "type": "scatter",
         "visible": true,
         "x": {
          "bdata": "jRsMvXGrBsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "3CjHoYKm7r8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Audience-Centric Natural Language Generation via<br>Style Infusion<br>ID: moorjani-etal-2022-audience<br>Subtopics: personalization<br>Year: 2022"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization",
         "showlegend": false,
         "type": "scatter",
         "visible": true,
         "x": {
          "bdata": "G1klQviu2r8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Jn6H+R/MCcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Knowledge of cultural moral norms in large<br>language models<br>ID: ramezani-xu-2023-knowledge<br>Subtopics: cultural<br>Year: 2023"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "P145c1EYCkA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "bvK0vnxj2j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Enabling Classifiers to Make Judgements Explicitly<br>Aligned with Human Values<br>ID: bang-etal-2023-enabling<br>Subtopics: diversity<br>Year: 2023"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "OeP4qyQ39T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "BBdvQnmUBkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Improving Factual Consistency for Knowledge-<br>Grounded Dialogue Systems via Knowledge<br>Enhancement and Alignment<br>ID: xue-etal-2023-improving<br>Subtopics: factuality<br>Year: 2023"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "5rEVjsrI0z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "avWx4VQBC0A=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Reliability Check: An Analysis of {GPT}-3{'}s<br>Response to Sensitive Topics and Prompt Wording<br>ID: khatun-brown-2023-reliability<br>Subtopics: general<br>Year: 2023",
          "Paper: Lauri Ingman at {S}em{E}val-2023 Task 4: A Chain<br>Classifier for Identifying Human Values behind<br>Arguments<br>ID: paulissen-wendt-2023-lauri<br>Subtopics: general<br>Year: 2023",
          "Paper: {R}eal{B}ehavior: A Framework for Faithfully<br>Characterizing Foundation Models' Human-like<br>Behavior Mechanisms<br>ID: zhou-etal-2023-realbehavior<br>Subtopics: general<br>Year: 2023",
          "Paper: {S}teer{LM}: Attribute Conditioned {SFT} as an<br>(User-Steerable) Alternative to {RLHF}<br>ID: dong-etal-2023-steerlm<br>Subtopics: general<br>Year: 2023",
          "Paper: Ethical Reasoning over Moral Alignment: A Case and<br>Framework for In-Context Ethical Policies in<br>{LLM}s<br>ID: rao-etal-2023-ethical<br>Subtopics: general<br>Year: 2023",
          "Paper: The Past, Present and Better Future of Feedback<br>Learning in Large Language Models for Subjective<br>Human Preferences and Values<br>ID: kirk-etal-2023-past<br>Subtopics: general<br>Year: 2023",
          "Paper: Learning Preference Model for {LLM}s via Automatic<br>Preference Data Generation<br>ID: huang-etal-2023-learning-preference<br>Subtopics: general<br>Year: 2023",
          "Paper: Axiomatic Preference Modeling for Longform<br>Question Answering<br>ID: rosset-etal-2023-axiomatic<br>Subtopics: general<br>Year: 2023",
          "Paper: Okapi: Instruction-tuned Large Language Models in<br>Multiple Languages with Reinforcement Learning<br>from Human Feedback<br>ID: lai-etal-2023-okapi<br>Subtopics: general<br>Year: 2023",
          "Paper: Probing Pre-Trained Language Models for Cross-<br>Cultural Differences in Values<br>ID: arora-etal-2023-probing<br>Subtopics: general<br>Year: 2023",
          "Paper: Towards Boosting the Open-Domain Chatbot with<br>Human Feedback<br>ID: lu-etal-2023-towards<br>Subtopics: general<br>Year: 2023"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "m4IR5lJ/978ilVAPNYv2v6aqSHe+Ofe/m3bahv68678kwQ4tH031v+c1Fx67Wea/LQzGbaV48b/Oq9wuL4nyv8fqcEE4RfK/8HUWEyNx9r9sGmayuVbzvw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "JmRy4oHjAUBdp8RVzDsEQE9+OqBoeAVAeyjuuOdRBUB1Z6JqXCsIQCZr5EiLkARAmD7QIKSwB0BqD02pCz8JQP42ja7BYwVAoqlsxP8ACUDaYC162a4BQA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {M}oral{D}ial: A Framework to Train and Evaluate<br>Moral Dialogue Systems via Moral Discussions<br>ID: sun-etal-2023-moraldial<br>Subtopics: moral<br>Year: 2023"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "ut2M+CMjBcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "N/tJK/of7r8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {D}ecipher{P}ref: Analyzing Influential Factors in<br>Human Preference Judgments via {GPT}-4<br>ID: hu-etal-2023-decipherpref<br>Subtopics: personalization<br>Year: 2023"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "Tub5Psok478=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "zIKf26suAsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "+brSr4UbBUCc+HcdYz3rP1jRZ01sV/o/zh3hm++Q9T/2JChKHtMBQEZrDH1pHfQ/7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/yYKg9OtY/j+3L8sIjwn+P2ZWqDklAQRAILDdcijk9j/fzdKbu+wHQKX+yAYm+AJA4K0b9o/SpD+RcQDvJwTgP9HtmU7rpwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "cLO6mEjm3L9DE4jbyEHLv+R/1ZsJyfm/+S8M262uzb/zaTTuiXXEv4J3oExKArI/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EpvcEYRs979gPcqrLhL0v1S5wZYDUfE/OC8Oju/Yx79XItQBdT7hPy4Hyhxz/PI/qhe7Ls3J8r8UA5eE7Nzwv9gT0Bs4ZfM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CDE}val: A Benchmark for Measuring the Cultural<br>Dimensions of Large Language Models<br>ID: wang-etal-2024-cdeval<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Are Generative Language Models Multicultural? A<br>Study on {H}ausa Culture and Emotions using<br>{C}hat{GPT}<br>ID: ahmad-etal-2024-generative<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Investigating Cultural Alignment of Large Language<br>Models<br>ID: alkhamissi-etal-2024-investigating<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: {S}usu Box or Piggy Bank: Assessing Cultural<br>Commonsense Knowledge between {G}hana and the {US}<br>ID: acquaye-etal-2024-susu<br>Subtopics: cultural, bias<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Musical Ethnocentrism in Large Language Models<br>ID: kruspe-2024-musical<br>Subtopics: cultural<br>Year: 2024",
          "Paper: Increasing the Difficulty of Automatically<br>Generated Questions via Reinforcement Learning<br>with Synthetic Preference for Cost-Effective<br>Cultural Heritage Dataset Generation<br>ID: thorne-etal-2024-increasing<br>Subtopics: cultural<br>Year: 2024",
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "hIA1I8nUCUATbQC2TfYFQPHvDI6ZygdA+brSr4UbBUCc+HcdYz3rP6kvnxZXS+0/vE+UQcwFBkBGawx9aR30P9/N0pu77AdAmkY144W2wz8ukdrqvSbAP5SbvIBpGuU/ywjFZvhyB0AZkExeVQQHQMwz13+YR98/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DYfPYIMN4D+1JXUurOjqP9HrOliJE9w/cLO6mEjm3L9DE4jbyEHLv6k9ZTq4Ge6/JWr2DZbv2L+Cd6BMSgKyP1ci1AF1PuE/AIDhItfw9L8Z5ISQq/XNv6WbrTlN1PA/L4ADB0wN8D8DT+Lg3+byP/RqqQamTPM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Whose Emotions and Moral Sentiments do Language<br>Models Reflect?<br>ID: he-etal-2024-whose<br>Subtopics: demographics<br>Year: 2024",
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: ``You Gotta be a Doctor, Lin'' : An Investigation<br>of Name-Based Bias of Large Language Models in<br>Employment Recommendations<br>ID: nghiem-etal-2024-gotta<br>Subtopics: demographics, bias<br>Year: 2024",
          "Paper: The Generation Gap: Exploring Age Bias in the<br>Value Systems of Large Language Models<br>ID: liu-etal-2024-generation-gap<br>Subtopics: demographics, bias<br>Year: 2024"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "37ti3escAkDOHeGb75D1P/YkKEoe0wFA7BWtG7G6AkAZSfoFA0cDQMGZ/d05xfc/XzNx38KHAEBmVqg5JQEEQNHtmU7rpwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "2Mqtr7Z8+j/5Lwzbra7Nv/NpNO6JdcS/td0UoNT94D/7pVTMvXyhvw69awAz7rq/EPW4m9P+A0BUucGWA1HxP9gT0Bs4ZfM/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {S}ea{LLM}s - Large Language Models for<br>{S}outheast {A}sia<br>ID: nguyen-etal-2024-seallms<br>Subtopics: diversity<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: Modular Pluralism: Pluralistic Alignment via<br>Multi-{LLM} Collaboration<br>ID: feng-etal-2024-modular<br>Subtopics: demographics, diversity<br>Year: 2024",
          "Paper: {GDPO}: Learning to Directly Align Language Models<br>with Diversity Using {GF}low{N}ets<br>ID: kwon-etal-2024-gdpo<br>Subtopics: diversity, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "/e7RShH9/z9CimAqrJL4P6fL2GGaS+U/XzNx38KHAECl/sgGJvgCQJSbvIBpGuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "C+AVVsDKA0Cwf7pCoD2pP8tFgAkMBtK/EPW4m9P+A0AuB8occ/zyP6WbrTlN1PA/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: Language Models are Alignable Decision-Makers:<br>Dataset and Application to the Medical Triage<br>Domain<br>ID: hu-etal-2024-language<br>Subtopics: ethical<br>Year: 2024"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "u8Hbly6B6z8D4dge8sPsPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "f7hHgEwF7L8J3yCJ4nkFQA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Learning to Trust Your Feelings: Leveraging Self-<br>awareness in {LLM}s for Hallucination Mitigation<br>ID: liang-etal-2024-learning<br>Subtopics: factuality<br>Year: 2024",
          "Paper: A Grounded Preference Model for {LLM} Alignment<br>ID: naseem-etal-2024-grounded<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: When to Trust {LLM}s: Aligning Confidence with<br>Response Quality<br>ID: tao-etal-2024-trust<br>Subtopics: factuality<br>Year: 2024",
          "Paper: {C}a{LM}: Contrasting Large and Small Language<br>Models to Verify Grounded Generation<br>ID: hsu-etal-2024-calm<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Reformatted Alignment<br>ID: fan-etal-2024-reformatted<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Knowledge Editing in Language Models via Adapted<br>Direct Preference Optimization<br>ID: rozner-etal-2024-knowledge<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Accuracy Paradox in {RLHF}: When Better Reward<br>Models Don{'}t Yield Better Language Models<br>ID: chen-etal-2024-accuracy<br>Subtopics: factuality<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Synchronous Faithfulness Monitoring for<br>Trustworthy Retrieval-Augmented Generation<br>ID: wu-etal-2024-synchronous<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Enhancing Language Model Factuality via<br>Activation-Based Confidence Calibration and Guided<br>Decoding<br>ID: liu-etal-2024-enhancing-language<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Evidence-Focused Fact Summarization for Knowledge-<br>Augmented Zero-Shot Question Answering<br>ID: ko-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Calibrating Language Models with Adaptive<br>Temperature Scaling<br>ID: xie-etal-2024-calibrating<br>Subtopics: factuality<br>Year: 2024",
          "Paper: Evidence-Driven Retrieval Augmented Response<br>Generation for Online Misinformation<br>ID: yue-etal-2024-evidence<br>Subtopics: factuality<br>Year: 2024"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "UD1B6TJC1D8ePW8rS3TgP6J9/MsE8uo/MenCm2d33D+rVNb7U13PP66J8sfmpsm/HhAsJ18j4z+CQKl/wFvqP67nBlvVzeI/ILDdcijk9j+t7YF0RFTIP7Q9PIvZjeI/K5kyQ6MJ0D9bk1ZH3tbRP+aoOn8iedY/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "iW0v9nr0B0BoE1/qmQAMQCK+V0fv09U/Cm5JtdzqBEDgs/SUAwgJQKtv9xNjIAdAvP7qKjvwBkCEuwd4PWvJP+e8a6L9+QpAOC8Oju/Yx7+sncr3SlwFQNb1FRiONQNA5T79m/l8CUD0ABqqTMEGQNpmEMkpYwVA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Preference-Guided Reflective Sampling for Aligning<br>Language Models<br>ID: ye-ng-2024-preference<br>Subtopics: general<br>Year: 2024",
          "Paper: Do {LLM}s Plan Like Human Writers? Comparing<br>Journalist Coverage of Press Releases with {LLM}s<br>ID: spangher-etal-2024-llms<br>Subtopics: general<br>Year: 2024",
          "Paper: The Greatest Good Benchmark: Measuring {LLM}s'<br>Alignment with Utilitarian Moral Dilemmas<br>ID: marraffini-etal-2024-greatest<br>Subtopics: general<br>Year: 2024",
          "Paper: Value Alignment from Unstructured Text<br>ID: padhi-etal-2024-value<br>Subtopics: general<br>Year: 2024",
          "Paper: Constructing Domain-Specific Evaluation Sets for<br>{LLM}-as-a-judge<br>ID: raju-etal-2024-constructing<br>Subtopics: general<br>Year: 2024",
          "Paper: Arithmetic Control of {LLM}s for Diverse User<br>Preferences: Directional Preference Alignment with<br>Multi-Objective Rewards<br>ID: wang-etal-2024-arithmetic<br>Subtopics: general<br>Year: 2024",
          "Paper: Whose Preferences? Differences in Fairness<br>Preferences and Their Impact on the Fairness of<br>{AI} Utilizing Human Feedback<br>ID: lerner-etal-2024-whose<br>Subtopics: general<br>Year: 2024",
          "Paper: Aligning Large Language Models with Human<br>Preferences through Representation Engineering<br>ID: liu-etal-2024-aligning<br>Subtopics: general<br>Year: 2024",
          "Paper: Unintended Impacts of {LLM} Alignment on Global<br>Representation<br>ID: ryan-etal-2024-unintended<br>Subtopics: general<br>Year: 2024",
          "Paper: {LIRE}: listwise reward enhancement for preference<br>alignment<br>ID: zhu-etal-2024-lire<br>Subtopics: general<br>Year: 2024",
          "Paper: Multi-Objective Linguistic Control of Large<br>Language Models<br>ID: nguyen-etal-2024-multi<br>Subtopics: general<br>Year: 2024",
          "Paper: Disentangling Length from Quality in Direct<br>Preference Optimization<br>ID: park-etal-2024-disentangling<br>Subtopics: general<br>Year: 2024",
          "Paper: Teaching Language Models to Self-Improve by<br>Learning from Language Feedback<br>ID: hu-etal-2024-teaching<br>Subtopics: general<br>Year: 2024",
          "Paper: {S}o{FA}: Shielded On-the-fly Alignment via<br>Priority Rule Following<br>ID: lu-etal-2024-sofa<br>Subtopics: general<br>Year: 2024",
          "Paper: Direct Preference Optimization with an Offset<br>ID: amini-etal-2024-direct<br>Subtopics: general<br>Year: 2024",
          "Paper: {C}ycle{A}lign: Iterative Distillation from Black-<br>box {LLM} to White-box Models for Better Human<br>Alignment<br>ID: hong-etal-2024-cyclealign<br>Subtopics: general<br>Year: 2024",
          "Paper: Eliminating Biased Length Reliance of Direct<br>Preference Optimization via Down-Sampled {KL}<br>Divergence<br>ID: lu-etal-2024-eliminating<br>Subtopics: general<br>Year: 2024",
          "Paper: {WPO}: Enhancing {RLHF} with Weighted Preference<br>Optimization<br>ID: zhou-etal-2024-wpo<br>Subtopics: general<br>Year: 2024",
          "Paper: {BPO}: Staying Close to the Behavior {LLM} Creates<br>Better Online {LLM} Alignment<br>ID: xu-etal-2024-bpo<br>Subtopics: general<br>Year: 2024",
          "Paper: {T}ele{C}hat: An Open-source Billingual Large<br>Language Model<br>ID: wang-etal-2024-telechat<br>Subtopics: general<br>Year: 2024",
          "Paper: {I}nstruct{E}val: Towards Holistic Evaluation of<br>Instruction-Tuned Large Language Models<br>ID: chia-etal-2024-instructeval<br>Subtopics: general<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: {Z}hu{J}iu-Knowledge: A Fairer Platform for<br>Evaluating Multiple Knowledge Types in Large<br>Language Models<br>ID: du-etal-2024-zhujiu<br>Subtopics: general<br>Year: 2024"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "jK/dR4Lv878JfXbHU7Lwv+P4uJjUAPO/LJ6A/crR9r8LW9N0hZfxv7Pzu+2bwvK/orfNLxsg8r8soRfhECf2v8+YbOhVSPe/hMLx+gJD9b9j/7VLZXX0v1CkQh2rQP+/kYQgEf3g87+KjHCZhOP5vwwuhaELpfK//2/KohDk479lsoWbeYD5vxMF9maNBfi/nwvSvC9c+r/qxIilOpTvvwDUFEf4DvS/hBiW3epb1b/WIyubNF/2vw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yAXDowx4CUDLeIV1IlsHQIF4S4bNIgZAFCbHB2tdBUDafEsVI2EFQJEB+fVVNgVA//cqW5auAkA/z1/rd6YFQIR1srx+5gNAYCJcJgmiBUAKp+pYMxADQGu91e2NawZA4M3ovq9kBUDywH/vccsDQJYPQ86quwVAaEZD4YmhBUBJsHO+9BAFQMRTUhwP/AVA8SJc7u2XB0D03X6tymwHQHJ7kdeZ0QVAgh9CU4uXv7+NRYE99yMHQA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024"
         ],
         "legendgroup": "hate",
         "marker": {
          "color": "rgb(51, 240, 151)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "hate",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "8ihDasTQwL8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5nGR2GOc3L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Are {U} a Joke Master? Pun Generation via Multi-<br>Stage Curriculum Learning towards a Humor {LLM}<br>ID: chen-etal-2024-u<br>Subtopics: humor<br>Year: 2024"
         ],
         "legendgroup": "humor",
         "marker": {
          "color": "rgb(87, 249, 118)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "humor",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "gMtL1Nzr/L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B/s58kDm/z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {A}ce{GPT}, Localizing Large Language Models in<br>{A}rabic<br>ID: huang-etal-2024-acegpt<br>Subtopics: language, cultural<br>Year: 2024"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "zDPXf5hH3z8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "9GqpBqZM8z8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Does Cross-Cultural Alignment Change the<br>Commonsense Morality of Language Models?<br>ID: jinnai-2024-cross<br>Subtopics: moral<br>Year: 2024",
          "Paper: Ethical Reasoning and Moral Value Alignment of<br>{LLM}s Depend on the Language We Prompt Them in<br>ID: agarwal-etal-2024-ethical<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Evaluating Moral Beliefs across {LLM}s through a<br>Pluralistic Framework<br>ID: liu-etal-2024-evaluating-moral<br>Subtopics: moral, cultural, bias<br>Year: 2024",
          "Paper: Story Morals: Surfacing value-driven narrative<br>schemas using large language models<br>ID: hobson-etal-2024-story<br>Subtopics: moral, cultural<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: {D}3{CODE}: Disentangling Disagreements in Data<br>across Cultures on Offensiveness Detection and<br>Evaluation<br>ID: mostafazadeh-davani-etal-2024-d3code<br>Subtopics: moral, cultural, diversity<br>Year: 2024",
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024",
          "Paper: Moral Disagreement over Serious Matters:<br>Discovering the Knowledge Hidden in the<br>Perspectives<br>ID: alvarez-nogales-araque-2024-moral<br>Subtopics: moral<br>Year: 2024",
          "Paper: {MOKA}: Moral Knowledge Augmentation for Moral<br>Event Extraction<br>ID: zhang-etal-2024-moka<br>Subtopics: moral<br>Year: 2024"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "p9p+aR4bB8Cc+HcdYz3rP0ZrDH1pHfQ/LpHa6r0mwD/grRv2j9KkP5SbvIBpGuU/kXEA7ycE4D/jaOKzJycHwGzjVjfuGgbA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "AO6PBqCT8L9DE4jbyEHLv4J3oExKArI/GeSEkKv1zb+qF7suzcnyv6WbrTlN1PA/FAOXhOzc8L/c1UuIQl7pv2IqctYZjve/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Reuse Your Rewards: Reward Model Transfer for<br>Zero-Shot Cross-Lingual Alignment<br>ID: wu-etal-2024-reuse<br>Subtopics: multilingual<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: {RLHF} Can Speak Many Languages: Unlocking<br>Multilingual Preference Optimization for {LLM}s<br>ID: dang-etal-2024-rlhf<br>Subtopics: multilingual<br>Year: 2024"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "D2OWvdCRBcCaRjXjhbbDPxIkcNwdnwTA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "e0x9hxia9r8AgOEi1/D0v9wDRCk5hfu/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Exploring Boundaries and Intensities in Offensive<br>and Hate Speech: Unveiling the Complex Spectrum of<br>Social Media Discourse<br>ID: ayele-etal-2024-exploring<br>Subtopics: offensiveness<br>Year: 2024"
         ],
         "legendgroup": "offensiveness",
         "marker": {
          "color": "rgb(247, 184, 54)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "offensiveness",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "uiV2QDb6AMA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Y0qHjfTmAcA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Evaluating Large Language Model Biases in Persona-<br>Steered Generation<br>ID: liu-etal-2024-evaluating-large<br>Subtopics: demographics, opinions, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: {LLM} Tropes: Revealing Fine-Grained Values and<br>Opinions in Large Language Models<br>ID: wright-etal-2024-llm<br>Subtopics: demographics, opinions, bias<br>Year: 2024"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "zh3hm++Q9T+PJVxY14rgP8GZ/d05xfc/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "+S8M262uzb8AUnINnQkFwA69awAz7rq/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {ABLE}: Personalized Disability Support with<br>Politeness and Empathy Integration<br>ID: mishra-etal-2024-able<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Bridging Cultural Nuances in Dialogue Agents<br>through Cultural Value Surveys<br>ID: cao-etal-2024-bridging<br>Subtopics: cultural, personalization<br>Year: 2024",
          "Paper: From Tarzan to {T}olkien: Controlling the Language<br>Proficiency Level of {LLM}s for Content Generation<br>ID: malik-etal-2024-tarzan<br>Subtopics: personalization<br>Year: 2024",
          "Paper: {BAPO}: Base-Anchored Preference Optimization for<br>Overcoming Forgetting in Large Language Models<br>Personalization<br>ID: lee-etal-2024-bapo<br>Subtopics: personalization<br>Year: 2024",
          "Paper: On Diversified Preferences of Large Language Model<br>Alignment<br>ID: zeng-etal-2024-diversified<br>Subtopics: diversity, personalization<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: A User-Centric Multi-Intent Benchmark for<br>Evaluating Large Language Models<br>ID: wang-etal-2024-user<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Learning Personalized Alignment for Evaluating<br>Open-ended Text Generation<br>ID: wang-etal-2024-learning-personalized<br>Subtopics: personalization<br>Year: 2024",
          "Paper: Detecting Mode Collapse in Language Models via<br>Narration<br>ID: hamilton-2024-detecting<br>Subtopics: personalization<br>Year: 2024"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "isHdv9EY7L+pL58WV0vtP046FQCCmuy/IaAv0pxQ5r+ny9hhmkvlP7vB25cuges/Rwg360aU5L/S1ayWdyTkvzgfK1wzNuu/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SBi03oq+CcCpPWU6uBnuv5qzIix1vgjAQJjsCfhNB8DLRYAJDAbSv3+4R4BMBey/EGM2ZO4yBcBSuPIFWCYKwJ3LWmjcxgbA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: How Gender Interacts with Political Values: A Case<br>Study on {C}zech {BERT} Models<br>ID: al-ali-libovicky-2024-gender<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024",
          "Paper: Hidden Persuaders: {LLM}s' Political Leaning and<br>Their Influence on Voters<br>ID: potter-etal-2024-hidden<br>Subtopics: political, bias<br>Year: 2024",
          "Paper: On the Relationship between Truth and Political<br>Bias in Language Models<br>ID: fulay-etal-2024-relationship<br>Subtopics: political, factuality, bias<br>Year: 2024",
          "Paper: Moral Foundations of Large Language Models<br>ID: abdulhai-etal-2024-moral<br>Subtopics: moral, political, bias<br>Year: 2024",
          "Paper: Beyond Prompt Brittleness: Evaluating the<br>Reliability and Consistency of Political<br>Worldviews in {LLM}s<br>ID: ceron-etal-2024-beyond<br>Subtopics: political<br>Year: 2024"
         ],
         "legendgroup": "political",
         "marker": {
          "color": "rgb(241, 95, 20)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "political",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "WNFnTWxX+j/JgqD061j+P7cvywiPCf4/ILDdcijk9j/grRv2j9KkP/ZjLVgfXuU/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "5H/VmwnJ+b8Sm9wRhGz3v2A9yqsuEvS/OC8Oju/Yx7+qF7suzcnyv7BlLtiJtAfA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Safety Arithmetic: A Framework for Test-time<br>Safety Alignment of Language Models by Steering<br>Parameters and Activations<br>ID: hazra-etal-2024-safety<br>Subtopics: safety<br>Year: 2024",
          "Paper: Gradient-Based Language Model Red Teaming<br>ID: wichers-etal-2024-gradient<br>Subtopics: safety<br>Year: 2024",
          "Paper: {RLHFP}oison: Reward Poisoning Attack for<br>Reinforcement Learning with Human Feedback in<br>Large Language Models<br>ID: wang-etal-2024-rlhfpoison<br>Subtopics: safety<br>Year: 2024",
          "Paper: Jailbreak Open-Sourced Large Language Models via<br>Enforced Decoding<br>ID: zhang-etal-2024-jailbreak<br>Subtopics: safety<br>Year: 2024",
          "Paper: {S}afe{D}ecoding: Defending against Jailbreak<br>Attacks via Safety-Aware Decoding<br>ID: xu-etal-2024-safedecoding<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Against Alignment-Breaking Attacks via<br>Robustly Aligned {LLM}<br>ID: cao-etal-2024-defending<br>Subtopics: safety<br>Year: 2024",
          "Paper: Course-Correction: Safety Alignment Using<br>Synthetic Preferences<br>ID: xu-etal-2024-course<br>Subtopics: safety<br>Year: 2024",
          "Paper: Intent-Aware and Hate-Mitigating Counterspeech<br>Generation via Dual-Discriminator Guided {LLM}s<br>ID: wang-etal-2024-intent<br>Subtopics: safety, hate<br>Year: 2024",
          "Paper: {UNIWIZ}: A Unified Large Language Model<br>Orchestrated Wizard for Safe Knowledge Grounded<br>Conversations<br>ID: das-srihari-2024-uniwiz<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: The Language Barrier: Dissecting Safety Challenges<br>of {LLM}s in Multilingual Contexts<br>ID: shen-etal-2024-language<br>Subtopics: safety<br>Year: 2024",
          "Paper: Adversarial Preference Optimization: Enhancing<br>Your Alignment via {RM}-{LLM} Game<br>ID: cheng-etal-2024-adversarial<br>Subtopics: safety<br>Year: 2024",
          "Paper: A Comprehensive Study of Jailbreak Attack versus<br>Defense for Large Language Models<br>ID: xu-etal-2024-comprehensive<br>Subtopics: safety<br>Year: 2024",
          "Paper: On the Vulnerability of Safety Alignment in Open-<br>Access {LLM}s<br>ID: yi-etal-2024-vulnerability<br>Subtopics: safety<br>Year: 2024",
          "Paper: Beyond One-Preference-Fits-All Alignment: Multi-<br>Objective Direct Preference Optimization<br>ID: zhou-etal-2024-beyond<br>Subtopics: safety, diversity<br>Year: 2024",
          "Paper: {C}ode{A}ttack: Revealing Safety Generalization<br>Challenges of Large Language Models via Code<br>Completion<br>ID: ren-etal-2024-codeattack<br>Subtopics: safety<br>Year: 2024",
          "Paper: Reasons to Reject? Aligning Language Models with<br>Judgments<br>ID: xu-etal-2024-reasons<br>Subtopics: safety<br>Year: 2024",
          "Paper: From Representational Harms to Quality-of-Service<br>Harms: A Case Study on Llama 2 Safety Safeguards<br>ID: chehbouni-etal-2024-representational<br>Subtopics: demographics, safety, bias<br>Year: 2024",
          "Paper: Negating Negatives: Alignment with Human Negative<br>Samples via Distributional Dispreference<br>Optimization<br>ID: duan-etal-2024-negating<br>Subtopics: safety<br>Year: 2024",
          "Paper: Exploring Multilingual Concepts of Human Values in<br>Large Language Models: Is Value Alignment<br>Consistent, Transferable and Controllable across<br>Languages?<br>ID: xu-etal-2024-exploring-multilingual<br>Subtopics: safety<br>Year: 2024",
          "Paper: Defending Large Language Models Against Jailbreak<br>Attacks via Layer-specific Editing<br>ID: zhao-etal-2024-defending-large<br>Subtopics: safety<br>Year: 2024",
          "Paper: {PURE}: Aligning {LLM} via Pluggable Query<br>Reformulation for Enhanced Helpfulness<br>ID: yao-etal-2024-pure<br>Subtopics: safety<br>Year: 2024",
          "Paper: A {LLM}-based Ranking Method for the Evaluation of<br>Automatic Counter-Narrative Generation<br>ID: zubiaga-etal-2024-llm<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Effective Counter-Responses: Aligning<br>Human Preferences with Strategies to Combat Online<br>Trolling<br>ID: lee-etal-2024-towards-effective<br>Subtopics: safety<br>Year: 2024",
          "Paper: Towards Tool Use Alignment of Large Language<br>Models<br>ID: chen-etal-2024-towards-tool<br>Subtopics: safety<br>Year: 2024",
          "Paper: Controllable Preference Optimization: Toward<br>Controllable Multi-Objective Alignment<br>ID: guo-etal-2024-controllable<br>Subtopics: safety, factuality<br>Year: 2024",
          "Paper: Alignment-Enhanced Decoding: Defending Jailbreaks<br>via Token-Level Adaptive Refining of Probability<br>Distributions<br>ID: liu-etal-2024-alignment<br>Subtopics: safety<br>Year: 2024",
          "Paper: The Multilingual Alignment Prism: Aligning Global<br>and Local Preferences to Reduce Harm<br>ID: aakanksha-etal-2024-multilingual<br>Subtopics: cultural, multilingual, safety<br>Year: 2024",
          "Paper: Holistic Automated Red Teaming for Large Language<br>Models through Top-Down Test Case Generation and<br>Multi-turn Interaction<br>ID: zhang-etal-2024-holistic<br>Subtopics: safety<br>Year: 2024",
          "Paper: Distract Large Language Models for Automatic<br>Jailbreak Attack<br>ID: xiao-etal-2024-distract<br>Subtopics: safety<br>Year: 2024",
          "Paper: Ensuring Safe and High-Quality Outputs: A<br>Guideline Library Approach for Language Models<br>ID: luo-etal-2024-ensuring<br>Subtopics: safety<br>Year: 2024",
          "Paper: {I}ter{A}lign: Iterative Constitutional Alignment<br>of Large Language Models<br>ID: chen-etal-2024-iteralign<br>Subtopics: safety<br>Year: 2024",
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: Stealthy and Persistent Unalignment on Large<br>Language Models via Backdoor Injections<br>ID: cao-etal-2024-stealthy<br>Subtopics: safety<br>Year: 2024",
          "Paper: Safer-Instruct: Aligning Language Models with<br>Automated Preference Data<br>ID: shi-etal-2024-safer<br>Subtopics: safety, general<br>Year: 2024",
          "Paper: Removing {RLHF} Protections in {GPT}-4 via Fine-<br>Tuning<br>ID: zhan-etal-2024-removing<br>Subtopics: safety<br>Year: 2024"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "nk7n0Las5j9xiX0Zm3DzPzweP/jwiu8/AsVbYsXi6D/pPWpGG2b0P1ON0yKQSek/koHy93eX4z/yKENqxNDAv6J9/MsE8uo/SCoT8EC35z8tCBnpkcP0P8ICHEziHeE/iN/scbm28T9CimAqrJL4Pz2ndAhZ3PI/vm3q/Alx6j/2JChKHtMBQHxuA/140eg/KjUSrM/M3T9UR6Jgdi/nP1Yytcxtg/Y/K39qGnff8D9+agJBL2HsP/bQN8ZlW9k/gkCpf8Bb6j+ptLsK5wvsP5pGNeOFtsM/tWUqR4uZ9D/JL1a4R07dP7LIqbptF/o/2A65493y1T80XT30aQ7/P8PakVCdW/g/2Tj/jDt++j+EGJbd6lvVv2t14QDByuc/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "BIe67rsUBMA/koOZeX4MwO7WsIDf/ATATvkeOCZuA8BIp7Hyn7AEwBeqJ6RpSgnAFc5qPuMzB8DmcZHYY5zcvyK+V0fv09U/ddLihrKiBcDErhGuV44IwOps2Vqg3wXAxF2EcohbB8Cwf7pCoD2pP+OpoxCkvQrAhxDndDZjBsDzaTTuiXXEv9NIXkoWtgPAHbDfsE4dBcCJCt7S17YFwJvPekdUCwnAsUGOwBORCMBwf2RZEi4GwAkE6PWqaQTAhLsHeD1ryT8x7BDAaqIHwACA4SLX8PS/NfoYYk1nB8A0Bp2J3BIGwDxAD/4FeAnA1C3XjpJoC8BZtxO+xVX8vyPDwtusKgHA/noXY/rWCMCCH0JTi5e/v5i9mLyABwXA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Adaptable Moral Stances of Large Language Models<br>on Sexist Content: Implications for Society and<br>Gender Discourse<br>ID: guo-etal-2024-adaptable<br>Subtopics: moral, sexism, bias<br>Year: 2024"
         ],
         "legendgroup": "sexism",
         "marker": {
          "color": "rgb(207, 48, 5)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "sexism",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "kXEA7ycE4D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "FAOXhOzc8L8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {CBBQ}: A {C}hinese Bias Benchmark Dataset Curated<br>with Human-{AI} Collaboration for Large Language<br>Models<br>ID: huang-xiong-2024-cbbq<br>Subtopics: cultural, social, bias<br>Year: 2024",
          "Paper: {P}op{ALM}: Popularity-Aligned Language Models for<br>Social Media Trendy Response Prediction<br>ID: yu-etal-2024-popalm<br>Subtopics: social<br>Year: 2024",
          "Paper: {K}or{NAT}: {LLM} Alignment Benchmark for {K}orean<br>Social Values and Common Knowledge<br>ID: lee-etal-2024-kornat<br>Subtopics: social, cultural<br>Year: 2024",
          "Paper: Rater Cohesion and Quality from a Vicarious<br>Perspective<br>ID: pandita-etal-2024-rater<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: The Potential and Challenges of Evaluating<br>Attitudes, Opinions, and Values in Large Language<br>Models<br>ID: ma-etal-2024-potential<br>Subtopics: social, opinions<br>Year: 2024",
          "Paper: From Pixels to Personas: Investigating and<br>Modeling Self-Anthropomorphism in Human-Robot<br>Dialogues<br>ID: li-etal-2024-pixels<br>Subtopics: social, ethical, personalization<br>Year: 2024",
          "Paper: {S}ocial{G}aze: Improving the Integration of Human<br>Social Norms in Large Language Models<br>ID: vijjini-etal-2024-socialgaze<br>Subtopics: demographics, social, bias<br>Year: 2024",
          "Paper: Systematic Biases in {LLM} Simulations of Debates<br>ID: taubenfeld-etal-2024-systematic<br>Subtopics: social, political, bias<br>Year: 2024"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "+brSr4UbBUAT1R7Lh7ICQLxPlEHMBQZA7BWtG7G6AkCPJVxY14rgP7vB25cuges/GUn6BQNHA0DJgqD061j+Pw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "cLO6mEjm3L8NOPTteaQBwCVq9g2W79i/td0UoNT94D8AUnINnQkFwH+4R4BMBey/+6VUzL18ob8Sm9wRhGz3vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Enhancing Reinforcement Learning with Dense<br>Rewards from Language Model Critic<br>ID: cao-etal-2024-enhancing<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Aligning Language Models with Textual<br>Feedback<br>ID: lloret-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Towards Healthy {AI}: Large Language Models Need<br>Therapists Too<br>ID: lin-etal-2024-towards<br>Subtopics: toxicity<br>Year: 2024",
          "Paper: Aligning as Debiasing: Causality-Aware Alignment<br>via Reinforcement Learning with Interventional<br>Feedback<br>ID: xia-etal-2024-aligning<br>Subtopics: toxicity, safety<br>Year: 2024",
          "Paper: A Multi-Aspect Framework for Counter Narrative<br>Evaluation using Large Language Models<br>ID: jones-etal-2024-multi<br>Subtopics: toxicity<br>Year: 2024"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "+MSlcwcNAUBmhIG1ykwFQENa+FvVRAZAw9qRUJ1b+D9mbUKFdxIDQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "zV67J3GV8r8DTayHHfj6v6no0JiYVfi/I8PC26wqAcBiB/BtHzD0vw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Flames: Benchmarking Value Alignment of {LLM}s in<br>{C}hinese<br>ID: huang-etal-2024-flames<br>Subtopics: safety, value<br>Year: 2024",
          "Paper: Value {FULCRA}: Mapping Large Language Models to<br>the Multidimensional Spectrum of Basic Human Value<br>ID: yao-etal-2024-value<br>Subtopics: value<br>Year: 2024"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "NF099GkO/z91lhohzvAHQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "WbcTvsVV/L+ZQKwG8zDnvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025"
         ],
         "legendgroup": "bias",
         "marker": {
          "color": "rgb(48, 18, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "bias",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "V3EABv8pBEA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "DgmoLW385D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025",
          "Paper: Self-Pluralising Culture Alignment for Large<br>Language Models<br>ID: xu-etal-2025-self<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Faux Polyglot: A Study on Information Disparity in<br>Multilingual Large Language Models<br>ID: sharma-etal-2025-faux<br>Subtopics: cultural, bias<br>Year: 2025",
          "Paper: {JAWAHER}: A Multidialectal Dataset of {A}rabic<br>Proverbs for {LLM} Benchmarking<br>ID: magdy-etal-2025-jawaher<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Towards Inclusive {A}rabic {LLM}s: A Culturally<br>Aligned Benchmark in {A}rabic Large Language Model<br>Evaluation<br>ID: nacar-etal-2025-towards<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CALM}: Unleashing the Cross-Lingual Self-Aligning<br>Ability of Language Model Question Answering<br>ID: wang-etal-2025-calm<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Cultural Alignment in Large Language Models: An<br>Explanatory Analysis Based on Hofstede{'}s<br>Cultural Dimensions<br>ID: masoud-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {CULTURALLY} {YOURS}: A Reading Assistant for<br>Cross-Cultural Content<br>ID: pandey-etal-2025-culturally<br>Subtopics: cultural<br>Year: 2025",
          "Paper: Cultural Learning-Based Culture Adaptation of<br>Language Models<br>ID: liu-etal-2025-cultural<br>Subtopics: cultural<br>Year: 2025",
          "Paper: {C}ul{F}i{T}: A Fine-grained Cultural-aware {LLM}<br>Training Paradigm via Multilingual Critique Data<br>Synthesis<br>ID: feng-etal-2025-culfit<br>Subtopics: cultural<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "cultural",
         "marker": {
          "color": "rgb(58, 48, 124)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "cultural",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "RZxupLCrqj/oY17dXiHNPwPGnnixiQhANHDesOSj+z9XcQAG/ykEQGKdPz1D5QVA63Heej3XBkCg0oOE0MYGQEWdLgofxwRAS8ovSqlNBUC1hjYl9wgKQEjvG2M1eQVA/u/AsyOk+T83xnKVqFf8Pw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yjWlqBBQ1L+wgNttXtvmv4ZfTeGhQ+k/KwPa2VZ07r8OCagtbfzkP5q+mdWW+t0/iX/Rp/HF8j/WuFBzoKj1P0otcnfXT+8/e1VYJC5a2D8SIUhlG4zqP24FhdsLV+U/egJp+kVE/D/xjVATp9fCvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Matina: A Culturally-Aligned {P}ersian Language<br>Model Using Multiple {L}o{RA} Experts<br>ID: hosseinbeigi-etal-2025-matina-culturally<br>Subtopics: culture<br>Year: 2025",
          "Paper: {LLM} Alignment for the {A}rabs: A Homogenous<br>Culture or Diverse Ones<br>ID: keleg-2025-llm<br>Subtopics: culture<br>Year: 2025",
          "Paper: Command {R}7{B} {A}rabic: a small, enterprise-<br>focused, multilingual, and culturally aware<br>{A}rabic {LLM}<br>ID: alnumay-etal-2025-command<br>Subtopics: culture<br>Year: 2025"
         ],
         "legendgroup": "culture",
         "marker": {
          "color": "rgb(66, 77, 182)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "culture",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "EM4aIy9vA0DswQzAEwEFQHHTDKIWZwVA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mhCIrLQf/j9c84c3N/H7PwwRD5Tl+f0/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025",
          "Paper: Rejected Dialects: Biases Against {A}frican<br>{A}merican Language in Reward Models<br>ID: mire-etal-2025-rejected<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Aligning to What? Limits to {RLHF} Based Alignment<br>ID: barnhart-etal-2025-aligning<br>Subtopics: demographics<br>Year: 2025",
          "Paper: ``You are Beautiful, Body Image Stereotypes are<br>Ugly!'' {BIS}tereo: A Benchmark to Measure Body<br>Image Stereotypes in Language Models<br>ID: asad-etal-2025-beautiful<br>Subtopics: demographics<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025"
         ],
         "legendgroup": "demographics",
         "marker": {
          "color": "rgb(69, 105, 220)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "demographics",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "DMeILJpm0T/091tiGy4DQEA8TomucAFA3LvoxVMpA0DuP8zA9aAAQA==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b9g81mAgDj9P7wJmtkjGf8/ijxL00k6A0BrtnrX1fyyvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025"
         ],
         "legendgroup": "diversity",
         "marker": {
          "color": "rgb(66, 132, 242)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "diversity",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "5Kc26Ino9T9mVWPLyD7DPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "SCZ6c4tswr/QMnByRROkvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {PROTECT}: Policy-Related Organizational Value<br>Taxonomy for Ethical Compliance and Trust<br>ID: mittal-etal-2025-protect<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Anak Baik: A Low-Cost Approach to Curate<br>{I}ndonesian Ethical and Unethical Instructions<br>ID: hakim-etal-2025-anak<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Chat Bankman-Fried: an Exploration of {LLM}<br>Alignment in Finance<br>ID: biancotti-etal-2025-chat<br>Subtopics: ethical<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "ethical",
         "marker": {
          "color": "rgb(58, 158, 251)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "ethical",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "nlovd09+4T9gRoNAKtv2P6ce8mTWZPE/N8ZylahX/D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "Zg4a2F54A0DDnX2AhxoHQJlTDZRbWgdA8Y1QE6fXwr8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Verifiable by Design: Aligning Language Models to<br>Quote from Pre-Training Data<br>ID: zhang-etal-2025-verifiable<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {PA}-{RAG}: {RAG} Alignment via Multi-Perspective<br>Preference Optimization<br>ID: wu-etal-2025-pa<br>Subtopics: factuality<br>Year: 2025",
          "Paper: Improving Model Factuality with Fine-grained<br>Critique-based Evaluator<br>ID: xie-etal-2025-improving<br>Subtopics: factuality<br>Year: 2025",
          "Paper: {L}o{GU}: Long-form Generation with Uncertainty<br>Expressions<br>ID: yang-etal-2025-logu<br>Subtopics: factuality<br>Year: 2025",
          "Paper: A Dual-Layered Evaluation of Geopolitical and<br>Cultural Bias in {LLM}s<br>ID: kim-kim-2025-dual<br>Subtopics: cultural, factuality<br>Year: 2025"
         ],
         "legendgroup": "factuality",
         "marker": {
          "color": "rgb(42, 184, 232)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "factuality",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "rjkDriqOsD8V5xLHeeu0P9Yu3tJf5dc/rv4SwP+Kw7/+78CzI6T5Pw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "05CmaWZxBkDGT/jJNqUJQF1YsLkMaAlAn9CbBPcYCEB6Amn6RUT8Pw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Context-{DPO}: Aligning Language Models for<br>Context-Faithfulness<br>ID: bi-etal-2025-context<br>Subtopics: faithfulness<br>Year: 2025"
         ],
         "legendgroup": "faithfulness",
         "marker": {
          "color": "rgb(28, 209, 208)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "faithfulness",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "40tGsN6z4L8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "egSax4s/BkA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Diverse {AI} Feedback For Large Language Model<br>Alignment<br>ID: yu-etal-2025-diverse<br>Subtopics: general<br>Year: 2025",
          "Paper: One fish, two fish, but not the whole sea:<br>Alignment reduces language models' conceptual<br>diversity<br>ID: murthy-etal-2025-one<br>Subtopics: general<br>Year: 2025",
          "Paper: Pipeline Analysis for Developing Instruct {LLM}s<br>in Low-Resource Languages: A Case Study on<br>{B}asque<br>ID: corral-etal-2025-pipeline<br>Subtopics: general<br>Year: 2025",
          "Paper: Sentimatic: Sentiment-guided Automatic Generation<br>of Preference Datasets for Customer Support<br>Dialogue System<br>ID: lee-han-2025-sentimatic<br>Subtopics: general<br>Year: 2025",
          "Paper: ({CPER}) From Guessing to Asking: An Approach to<br>Resolving Persona Knowledge Gap in {LLM}s during<br>Multi-Turn Conversations<br>ID: baskar-etal-2025-cper<br>Subtopics: general<br>Year: 2025",
          "Paper: How Inclusively do {LM}s Perceive Social and Moral<br>Norms?<br>ID: galarnyk-etal-2025-inclusively<br>Subtopics: general<br>Year: 2025",
          "Paper: {M}eta{A}lign: Align Large Language Models with<br>Diverse Preferences during Inference Time<br>ID: zhang-etal-2025-metaalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {SPICA}: Retrieving Scenarios for Pluralistic In-<br>Context Alignment<br>ID: chen-etal-2025-spica<br>Subtopics: general<br>Year: 2025",
          "Paper: Self-Steering Optimization: Autonomous Preference<br>Optimization for Large Language Models<br>ID: xiang-etal-2025-self<br>Subtopics: general<br>Year: 2025",
          "Paper: Well Begun is Half Done: Low-resource Preference<br>Alignment by Weak-to-Strong Decoding<br>ID: song-etal-2025-well<br>Subtopics: general<br>Year: 2025",
          "Paper: ``{I} understand your perspective'': {LLM}<br>Persuasion through the Lens of Communicative<br>Action Theory<br>ID: donmez-falenska-2025-understand<br>Subtopics: general<br>Year: 2025",
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025",
          "Paper: Multi-perspective Preference Alignment of {LLM}s<br>for Programming-Community Question Answering<br>ID: yang-etal-2025-multi<br>Subtopics: general<br>Year: 2025",
          "Paper: Aligning Large Language Models with Human Opinions<br>through Persona Selection and<br>Value{--}Belief{--}Norm Reasoning<br>ID: do-etal-2025-aligning<br>Subtopics: general<br>Year: 2025",
          "Paper: {COF}: Adaptive Chain of Feedback for Comparative<br>Opinion Quintuple Extraction<br>ID: xu-etal-2025-cof<br>Subtopics: general<br>Year: 2025",
          "Paper: Is my Meeting Summary Good? Estimating Quality<br>with a Multi-{LLM} Evaluator<br>ID: kirstein-etal-2025-meeting<br>Subtopics: general<br>Year: 2025",
          "Paper: Bias in the Mirror : Are {LLM}s opinions robust to<br>their own adversarial attacks<br>ID: rennard-etal-2025-bias<br>Subtopics: general<br>Year: 2025",
          "Paper: Semantic-Eval : A Semantic Comprehension<br>Evaluation Framework for Large Language Models<br>Generation without Training<br>ID: li-etal-2025-semantic-eval<br>Subtopics: general<br>Year: 2025",
          "Paper: Frictional Agent Alignment Framework: Slow Down<br>and Don{'}t Break Things<br>ID: nath-etal-2025-frictional<br>Subtopics: general<br>Year: 2025",
          "Paper: Gradient-Adaptive Policy Optimization: Towards<br>Multi-Objective Alignment of Large Language Models<br>ID: li-etal-2025-gradient<br>Subtopics: general<br>Year: 2025",
          "Paper: Cheems: A Practical Guidance for Building and<br>Evaluating {C}hinese Reward Models from Scratch<br>ID: wen-etal-2025-cheems<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}rox{A}nn: Use-Oriented Evaluations of Topic<br>Models and Document Clustering<br>ID: hoyle-etal-2025-proxann<br>Subtopics: general<br>Year: 2025",
          "Paper: {C}riti{Q}: Mining Data Quality Criteria from<br>Human Preferences<br>ID: guo-etal-2025-critiq<br>Subtopics: general<br>Year: 2025",
          "Paper: {A}uto{M}ix{A}lign: Adaptive Data Mixing for<br>Multi-Task Preference Optimization in {LLM}s<br>ID: corrado-etal-2025-automixalign<br>Subtopics: general<br>Year: 2025",
          "Paper: {BIG}5-{CHAT}: Shaping {LLM} Personalities Through<br>Training on Human-Grounded Data<br>ID: li-etal-2025-big5<br>Subtopics: general<br>Year: 2025",
          "Paper: {VITAL}: A New Dataset for Benchmarking<br>Pluralistic Alignment in Healthcare<br>ID: shetty-etal-2025-vital<br>Subtopics: general<br>Year: 2025",
          "Paper: {D}e{AL}: Decoding-time Alignment for Large<br>Language Models<br>ID: huang-etal-2025-deal<br>Subtopics: general<br>Year: 2025",
          "Paper: Bone Soups: A Seek-and-Soup Model Merging Approach<br>for Controllable Multi-Objective Generation<br>ID: xie-etal-2025-bone<br>Subtopics: general<br>Year: 2025",
          "Paper: {P}op{A}lign: Diversifying Contrasting Patterns<br>for a More Comprehensive Alignment<br>ID: wang-etal-2025-popalign<br>Subtopics: general<br>Year: 2025",
          "Paper: $\\mathcal{A}^3$: Automatic Alignment Framework for<br>Attributed Text Generation<br>ID: wang-etal-2025-a3<br>Subtopics: general<br>Year: 2025",
          "Paper: {RUBRIC}-{MQM} : Span-Level {LLM}-as-judge in<br>Machine Translation For High-End Models<br>ID: kim-2025-rubric<br>Subtopics: general<br>Year: 2025"
         ],
         "legendgroup": "general",
         "marker": {
          "color": "rgb(33, 226, 181)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "general",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "9q2Hx9B2+7/lcQMSkzP8v5RntCh/nPW/PUKuzH3Z8b8L/7ETdxn0v6Iypkd81fW/uYsR+rYt8r/ecQEMcwX3v3kPN/2rJOi/Gdq8/DTM8b+6L4pL1Gfzv9cDHpUMOQHAiezMvxoa678kSyjrSRz3v17pgHP7jvK/ZHxmntR567/AM9pey8T3v8bEnhigEva/IqkWa3258r/Ype03YOr3vw9UO7H0Ife/gGObLUFd8r+tI7SxhsDmv8qQZ0t3wv2/sX4YuObu87/qenqINrT3vxvuC+tlave/V6xuINDC9788vXkw7HH1v/r9F1RBSO2/0On8aG+R978=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "B3MRA+xtBkAVnTTAnRoHQEDkdH/ICAlA8qMVvhvHA0Cb24rNxZAGQLDCXOaJOgdAtfbxX24xAkBJW9Rbkb4BQK2WLz2jdgZAQ+k97NXlB0D/8vG7EuAGQLi2O5IlkPQ/UGE9ZHFpB0D1XXYJQK0CQPAUuz/xuwNAbTz5tgUUBkBLGKzmHnEGQP4/rYGklAhA2hQQEM8DCkC6w8f6NJ8DQIRyvTe7QQVAjDqdKXEEB0D1s4fcLPAGQPyu5zgJygNA9i6vKt+TCEBFrcVCayMEQFXrn6ftDAVArZlsUxeNBUB4Uk0bCHsAQD8L9jMkCAdA27uSzxd5A0A=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Implicit Cross-Lingual Rewarding for Efficient<br>Multilingual Preference Alignment<br>ID: yang-etal-2025-implicit<br>Subtopics: language<br>Year: 2025",
          "Paper: 7 Points to {T}singhua but 10 Points to ?<br>Assessing Large Language Models in Agentic<br>Multilingual National Bias<br>ID: liu-etal-2025-7<br>Subtopics: language<br>Year: 2025",
          "Paper: {REPA}: {R}ussian Error Types Annotation for<br>Evaluating Text Generation and Judgment<br>Capabilities<br>ID: pugachev-etal-2025-repa<br>Subtopics: language<br>Year: 2025"
         ],
         "legendgroup": "language",
         "marker": {
          "color": "rgb(125, 252, 88)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "language",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "bYnGRpGxBcD3jsB1vOEIwGvI3T+kRQrA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "OfDLgE9p9z/jhEnQj07pP8m5TL6pg/E/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Aligning Language Models for {Icelandic} Legal<br>Text Summarization<br>ID: hardarson-etal-2025-aligning<br>Subtopics: legal<br>Year: 2025"
         ],
         "legendgroup": "legal",
         "marker": {
          "color": "rgb(164, 252, 59)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "legal",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "5Ygo2nlACsA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ent4WHOe4j8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {MWPO}: Enhancing {LLM}s Performance through<br>Multi-Weight Preference Strength and Length<br>Optimization<br>ID: xu-etal-2025-mwpo<br>Subtopics: length, general<br>Year: 2025"
         ],
         "legendgroup": "length",
         "marker": {
          "color": "rgb(190, 240, 55)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "length",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "1wMelQw5AcA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "uLY7kiWQ9D8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {UA}lign: {LLM} Alignment Benchmark for the<br>{U}krainian Language<br>ID: kravchenko-etal-2025-ualign<br>Subtopics: moral<br>Year: 2025",
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: What Counts Underlying {LLM}s' Moral Dilemma<br>Judgments?<br>ID: wu-deng-2025-counts<br>Subtopics: moral<br>Year: 2025",
          "Paper: {HISTOIRESMORALES}: A {F}rench Dataset for<br>Assessing Moral Alignment<br>ID: leteno-etal-2025-histoiresmorales<br>Subtopics: moral, cultural<br>Year: 2025",
          "Paper: Probabilistic Aggregation and Targeted Embedding<br>Optimization for Collective Moral Reasoning in<br>Large Language Models<br>ID: yuan-etal-2025-probabilistic<br>Subtopics: moral<br>Year: 2025",
          "Paper: Comparing Moral Values in {W}estern {E}nglish-<br>speaking societies and {LLM}s with Word<br>Associations<br>ID: xiang-etal-2025-comparing<br>Subtopics: moral<br>Year: 2025",
          "Paper: Deontological Keyword Bias: The Impact of Modal<br>Expressions on Normative Judgments of Language<br>Models<br>ID: park-etal-2025-deontological<br>Subtopics: moral<br>Year: 2025",
          "Paper: Exploring {LLM}s' Ability to Spontaneously and<br>Conditionally Modify Moral Expressions through<br>Text Manipulation<br>ID: greco-etal-2025-exploring<br>Subtopics: moral<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025"
         ],
         "legendgroup": "moral",
         "marker": {
          "color": "rgb(215, 226, 53)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "moral",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "guXwweDsB8A1sLDmfmfqv5a/iWkpDArARZxupLCrqj9NeH1sDQ4JwB+oTEyixgjA++lfvlWTB8DtOKBteFgFwOhpbxaIy4Q/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "HKWOKGSM6b+z0eLfxCj9v+rVv+RyNfG/yjWlqBBQ1L8D3KQg7XHyv545VrqK7vW/BfsXxBnQ9b80R9Kow2Tuv19Iq/BS8u2/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Got Compute, but No Data: {Lessons} From Post-<br>training a {Finnish} {LLM}<br>ID: zosa-etal-2025-got<br>Subtopics: multilingual<br>Year: 2025",
          "Paper: High-Dimension Human Value Representation in Large<br>Language Models<br>ID: cahyawijaya-etal-2025-high<br>Subtopics: cultural, multilingual<br>Year: 2025"
         ],
         "legendgroup": "multilingual",
         "marker": {
          "color": "rgb(235, 206, 57)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "multilingual",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "0tbsJn3S2b967JcFJP4EwOhjXt1eIc0/",
          "dtype": "f8"
         },
         "y": {
          "bdata": "fdZQcHLuAcBspUJEqM3+v7CA221e2+a/",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Distributional Alignment of Large<br>Language Models<br>ID: meister-etal-2025-benchmarking<br>Subtopics: opinions, demographics<br>Year: 2025"
         ],
         "legendgroup": "opinions",
         "marker": {
          "color": "rgb(253, 159, 46)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "opinions",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "DMeILJpm0T8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "ClRTexBp4b8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {C}om{PO}: Community Preferences for Language<br>Model Personalization<br>ID: kumar-etal-2025-compo<br>Subtopics: personalization, diversity<br>Year: 2025",
          "Paper: Beyond Excess and Deficiency: Adaptive Length Bias<br>Mitigation in Reward Models for {RLHF}<br>ID: bu-etal-2025-beyond<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-judge: Personalized Alignment of Large<br>Language Models via Token-level Self-judgment<br>ID: zhang-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: A Survey on Personalized {A}lignment{---}{T}he<br>Missing Piece for Large Language Models in Real-<br>World Applications<br>ID: guan-etal-2025-survey<br>Subtopics: personalization<br>Year: 2025",
          "Paper: The Reader is the Metric: How Textual Features and<br>Reader Profiles Explain Conflicting Evaluations of<br>{AI} Creative Writing<br>ID: marco-etal-2025-reader<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Enhancing Persona Consistency for {LLM}s' Role-<br>Playing using Persona-Aware Contrastive Learning<br>ID: ji-etal-2025-enhancing<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Disentangling Preference Representation and Text<br>Generation for Efficient Individual Preference<br>Alignment<br>ID: zhang-etal-2025-disentangling<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Persona-Consistent Dialogue Generation via Pseudo<br>Preference Tuning<br>ID: takayama-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Aligning {LLM}s with Individual Preferences via<br>Interaction<br>ID: wu-etal-2025-aligning<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Engagement-driven Persona Prompting for Rewriting<br>News Tweets<br>ID: gopalakrishna-pillai-etal-2025-engagement<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {PERSONA}: A Reproducible Testbed for Pluralistic<br>Alignment<br>ID: castricato-etal-2025-persona<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Using {LLM}s to improve {RL} policies in<br>personalized health adaptive interventions<br>ID: karine-marlin-2025-using<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {MAPS}: Motivation-Aware Personalized Search via<br>{LLM}-Driven Consultation Alignment<br>ID: qin-etal-2025-maps<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Whose Boat Does it Float? Improving<br>Personalization in Preference Tuning via Inferred<br>User Personas<br>ID: balepur-etal-2025-whose<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Know You First and Be You Better: Modeling Human-<br>Like User Simulators via Implicit Profiles<br>ID: wang-etal-2025-know<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Evaluating Personalized Tool-Augmented {LLM}s from<br>the Perspectives of Personalization and<br>Proactivity<br>ID: hao-etal-2025-evaluating<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Comparison-based Active Preference Learning for<br>Multi-dimensional Personalization<br>ID: oh-etal-2025-comparison<br>Subtopics: personalization<br>Year: 2025",
          "Paper: {G}reater{P}rompt: A Unified, Customizable, and<br>High-Performing Open-Source Toolkit for Prompt<br>Optimization<br>ID: zheng-etal-2025-greaterprompt<br>Subtopics: personalization<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "personalization",
         "marker": {
          "color": "rgb(248, 127, 33)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "personalization",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "sHjlvk+vlD9mVWPLyD7DP6/8Wm+WauG/qlR8LLytyL+8ZSXNZhDwvwaJc2oVJMy/AxSHpS2M0L9Op4qjnFDfv8/PiIfpBde/nl3lZDHZ4r+/qU2ADPLgv+XQ5C1rs+K/FKGBzvEZ8L+X7fKZcV/iv+QPFvI3Cum/skpBMo+uxr+khnHPexTYv2Atf8zjsOa/NKILXDF94r83xnKVqFf8Pw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NBE39UeCBsDQMnByRROkv3tvtnTB/wjAPjuJb3rhDMCVAjNpA6AJwBH9HWxUJQfAQCXYenRmCcBsOyBbTMQDwCES3s2DIwnAXEwJCZdJCMDyi3YJ9soFwPK7OUnDqQPAuwbvQTdQCcDv/bbO/jwGwGV/BnWM6QTAiBvXAQMtCMC4qlC1p4gGwNF55GeF1AbAkCYSxkajCcDxjVATp9fCvw==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Smaller Large Language Models Can Do Moral Self-<br>Correction<br>ID: liu-etal-2025-smaller<br>Subtopics: moral, safety<br>Year: 2025",
          "Paper: {I}nstruction{CP}: A Simple yet Effective Approach<br>for Transferring Large Language Models to Target<br>Languages<br>ID: chen-etal-2025-instructioncp<br>Subtopics: safety, multilingual<br>Year: 2025",
          "Paper: Unlocking Decoding-time Controllability: Gradient-<br>Free Multi-Objective Alignment with Contrastive<br>Prompts<br>ID: fu-etal-2025-unlocking<br>Subtopics: safety, personalization<br>Year: 2025",
          "Paper: {S}eq{AR}: Jailbreak {LLM}s with Sequential Auto-<br>Generated Characters<br>ID: yang-etal-2025-seqar<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DPL}: Diverse Preference Learning Without A<br>Reference Model<br>ID: nath-etal-2025-dpl<br>Subtopics: safety, diversity<br>Year: 2025",
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025",
          "Paper: Navigating the Cultural Kaleidoscope: A<br>Hitchhiker{'}s Guide to Sensitivity in Large<br>Language Models<br>ID: banerjee-etal-2025-navigating<br>Subtopics: cultural, safety<br>Year: 2025",
          "Paper: Multilingual Blending: Large Language Model Safety<br>Alignment Evaluation with Language Mixture<br>ID: song-etal-2025-multilingual<br>Subtopics: safety<br>Year: 2025",
          "Paper: An Optimizable Suffix Is Worth A Thousand<br>Templates: Efficient Black-box Jailbreaking<br>without Affirmative Phrases via {LLM} as Optimizer<br>ID: jiang-etal-2025-optimizable<br>Subtopics: safety<br>Year: 2025",
          "Paper: Adversarial Preference Learning for Robust {LLM}<br>Alignment<br>ID: wang-etal-2025-adversarial<br>Subtopics: safety<br>Year: 2025",
          "Paper: Towards Safety Reasoning in {LLM}s: {AI}-agentic<br>Deliberation for Policy-embedded {C}o{T} Data<br>Creation<br>ID: kumarage-etal-2025-towards<br>Subtopics: safety<br>Year: 2025",
          "Paper: {DIESEL}: A Lightweight Inference-Time Safety<br>Enhancement for Language Models<br>ID: ganon-etal-2025-diesel<br>Subtopics: safety<br>Year: 2025",
          "Paper: Don{'}t Say No: Jailbreaking {LLM} by Suppressing<br>Refusal<br>ID: zhou-etal-2025-dont<br>Subtopics: safety<br>Year: 2025",
          "Paper: Intention Analysis Makes {LLM}s A Good Jailbreak<br>Defender<br>ID: zhang-etal-2025-intention<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unraveling the Mystery: Defending Against<br>Jailbreak Attacks Via Unearthing Real Intention<br>ID: li-etal-2025-unraveling<br>Subtopics: safety<br>Year: 2025",
          "Paper: Mixture of insigh{T}ful Experts ({M}o{TE}): The<br>Synergy of Reasoning Chains and Expert Mixtures in<br>Self-Alignment<br>ID: liu-etal-2025-mixture<br>Subtopics: safety<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Safer or Luckier? {LLM}s as Safety Evaluators Are<br>Not Robust to Artifacts<br>ID: chen-goldfarb-tarrant-2025-safer<br>Subtopics: safety<br>Year: 2025",
          "Paper: Small Changes, Big Impact: How Manipulating a Few<br>Neurons Can Drastically Alter {LLM} Aggression<br>ID: lee-etal-2025-small<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MPO}: Multilingual Safety Alignment via Reward<br>Gap Optimization<br>ID: zhao-etal-2025-mpo<br>Subtopics: safety<br>Year: 2025",
          "Paper: {MTSA}: Multi-turn Safety Alignment for {LLM}s<br>through Multi-round Red-teaming<br>ID: guo-etal-2025-mtsa<br>Subtopics: safety<br>Year: 2025",
          "Paper: {LSSF}: Safety Alignment for Large Language Models<br>through Low-Rank Safety Subspace Fusion<br>ID: zhou-etal-2025-lssf<br>Subtopics: safety<br>Year: 2025",
          "Paper: Efficient Safety Alignment of Large Language<br>Models via Preference Re-ranking and<br>Representation-based Reward Modeling<br>ID: qiyuan-etal-2025-efficient<br>Subtopics: safety<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: {PKU}-{S}afe{RLHF}: Towards Multi-Level Safety<br>Alignment for {LLM}s with Human Preference<br>ID: ji-etal-2025-pku<br>Subtopics: safety<br>Year: 2025",
          "Paper: Guardrails and Security for {LLM}s: Safe, Secure<br>and Controllable Steering of {LLM} Applications<br>ID: rebedea-etal-2025-guardrails<br>Subtopics: safety<br>Year: 2025"
         ],
         "legendgroup": "safety",
         "marker": {
          "color": "rgb(226, 70, 11)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "safety",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "NbCw5n5n6r/S1uwmfdLZv7B45b5Pr5Q/OzKllS126z/kpzboiej1P+q7Fbdj1ABANHDesOSj+z+A7QD3TmTxP5SLeO3KouY/GyY4/uzY+j/38eLX65zxPxCSb/NcNeA/+SSI72bg8T80BGdZsj/0P0JcjOGgb/M/hxyU8D8I6T+bCRxjw0H3P24ioIrc4eU/AQ6WCQtU8D+C3lFbx6fgP053uHfDM+w/yfNN6T+f8T+2k0AAflncPzqM0MQ/SgJABX+GD9ih8z/SfB1W0bTyPw==",
          "dtype": "f8"
         },
         "y": {
          "bdata": "s9Hi38Qo/b991lBwcu4BwDQRN/VHggbAeHFKWdZVBMBIJnpzi2zCv8R0grKmyQLAKwPa2VZ07r/q7qmDAiILwKIUI9CxVwrAu/EiA+tDA8AeUkQ5bm0EwO5JsMeG+wPAS99vvfvZBsCPt+Z1QegGwH7voRrDkAjA4TPxzAAnBMAPAdUE8Tj9v6m3L5aqqwjAYIdO3FzfB8Cs85vkPesGwEbBlVuAxAfADh+wYny5BcDQB5pq1hUFwJSdMIhdcPm/m/ZLBlugA8AYoB9T50ENwA==",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Do Large Language Models Learn Human-Like<br>Strategic Preferences?<br>ID: roberts-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Team Conversational {AI}: Introducing Effervesce<br>ID: skenderi-etal-2025-team<br>Subtopics: social<br>Year: 2025",
          "Paper: {G}uide{LLM}: Exploring {LLM}-Guided Conversation<br>with Applications in Autobiography Interviewing<br>ID: duan-etal-2025-guidellm<br>Subtopics: social<br>Year: 2025",
          "Paper: {R}esearch{A}gent: Iterative Research Idea<br>Generation over Scientific Literature with Large<br>Language Models<br>ID: baek-etal-2025-researchagent<br>Subtopics: social<br>Year: 2025",
          "Paper: Northeastern Uni at Multilingual Counterspeech<br>Generation: Enhancing Counter Speech Generation<br>with {LLM} Alignment through Direct Preference<br>Optimization<br>ID: wadhwa-etal-2025-northeastern<br>Subtopics: social<br>Year: 2025",
          "Paper: Large Language Models with Reinforcement Learning<br>from Human Feedback Approach for Enhancing<br>Explainable Sexism Detection<br>ID: riahi-samani-etal-2025-large<br>Subtopics: social<br>Year: 2025",
          "Paper: Hire Me or Not? Examining Language Model{'}s<br>Behavior with Occupation Attributes<br>ID: zhang-etal-2025-hire<br>Subtopics: social<br>Year: 2025",
          "Paper: Veracity Bias and Beyond: Uncovering {LLM}s'<br>Hidden Beliefs in Problem-Solving Reasoning<br>ID: zhou-di-eugenio-2025-veracity<br>Subtopics: social, demographics<br>Year: 2025",
          "Paper: Aligned but Blind: Alignment Increases Implicit<br>Bias by Reducing Awareness of Race<br>ID: sun-etal-2025-aligned<br>Subtopics: social<br>Year: 2025",
          "Paper: Think Again! The Effect of Test-Time Compute on<br>Preferences, Opinions, and Beliefs of Large<br>Language Models<br>ID: kour-etal-2025-think<br>Subtopics: social, cultural, ethical, personalization<br>Year: 2025"
         ],
         "legendgroup": "social",
         "marker": {
          "color": "rgb(184, 30, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "social",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "AIBbg39lAkBPX1iWMIUBQIOu6ZdqHAJAKA2bp23VA0CdCTzh6nADQCAL/a0WOgBACDFJxSc/AUDuP8zA9aAAQD74QKlDsAJAN8ZylahX/D8=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "NqRd9CWCAsAHQP4Jf7D+v1Dek8Kbi/6/bo/JFrWC+L9MdaLGQV4FwIYhTGVDNP2/WuoepP+Z+L9rtnrX1fyyv28w9T+h0/y/8Y1QE6fXwr8=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Model Surgery: Modulating {LLM}{'}s Behavior Via<br>Simple Parameter Editing<br>ID: wang-etal-2025-model<br>Subtopics: safety, toxicity<br>Year: 2025"
         ],
         "legendgroup": "toxicity",
         "marker": {
          "color": "rgb(154, 16, 1)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "toxicity",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "6rsVt2PUAEA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "xHSCsqbJAsA=",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Paper: Benchmarking Multi-National Value Alignment for<br>Large Language Models<br>ID: ju-etal-2025-benchmarking<br>Subtopics: value<br>Year: 2025",
          "Paper: Are the Values of {LLM}s Structurally Aligned with<br>Humans? A Causal Perspective<br>ID: kang-etal-2025-values<br>Subtopics: value<br>Year: 2025",
          "Paper: Do language models practice what they preach?<br>Examining language ideologies about gendered<br>language reform encoded in {LLM}s<br>ID: watson-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: {CONTRANS}: Weak-to-Strong Alignment Engineering<br>via Concept Transplantation<br>ID: dong-etal-2025-contrans<br>Subtopics: value<br>Year: 2025",
          "Paper: What{'}s the most important value? {INVP}:<br>{IN}vestigating the Value Priorities of {LLM}s<br>through Decision-making in Social Scenarios<br>ID: liu-etal-2025-whats<br>Subtopics: value<br>Year: 2025",
          "Paper: Can Language Models Reason about Individualistic<br>Human Values and Preferences?<br>ID: jiang-etal-2025-language<br>Subtopics: value<br>Year: 2025",
          "Paper: Generative Psycho-Lexical Approach for<br>Constructing Value Systems in Large Language<br>Models<br>ID: ye-etal-2025-generative<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Portrait: Assessing Language Models' Values<br>through Psychometrically and Ecologically Valid<br>Items<br>ID: han-etal-2025-value<br>Subtopics: value<br>Year: 2025",
          "Paper: Mining the uncertainty patterns of humans and<br>models in the annotation of moral foundations and<br>human values<br>ID: falk-lapesa-2025-mining<br>Subtopics: moral, value<br>Year: 2025",
          "Paper: Internal Value Alignment in Large Language Models<br>through Controlled Value Vector Activation<br>ID: jin-etal-2025-internal<br>Subtopics: value<br>Year: 2025",
          "Paper: Towards Better Value Principles for Large Language<br>Model Alignment: A Systematic Evaluation and<br>Enhancement<br>ID: xu-etal-2025-towards<br>Subtopics: value<br>Year: 2025",
          "Paper: Unintended Harms of Value-Aligned {LLM}s:<br>Psychological and Empirical Insights<br>ID: choi-etal-2025-unintended<br>Subtopics: value, safety<br>Year: 2025",
          "Paper: Value Compass Benchmarks: A Comprehensive,<br>Generative and Self-Evolving Platform for {LLM}s'<br>Value Evaluation<br>ID: yao-etal-2025-value<br>Subtopics: value<br>Year: 2025"
         ],
         "legendgroup": "value",
         "marker": {
          "color": "rgb(122, 4, 2)",
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "size": 12
         },
         "mode": "markers",
         "name": "value",
         "showlegend": false,
         "type": "scatter",
         "visible": false,
         "x": {
          "bdata": "ujPYtG/NB0C8xck9hMAGQFETE2bzOQpARwZ1U9kdCECNKG+j4ToLQFO18OUrxAVAmwkcY8NB9z9IubxMvLcCQOhpbxaIy4Q/QecNf6BODEA8DkGDz5AHQDqM0MQ/SgJAK1CArz/+B0A=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "yozrR+1F5r9Lv1qncSnmvx7knBwX5ea/JT5aPPJz3b+UCFwPsVXZv8gtw3thNO2/DwHVBPE4/b89o82+eub0v19Iq/BS8u2/9IvMg7Mozr8IA3DfE+3tv5SdMIhdcPm///apLdSu778=",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "showlegend": true,
        "sliders": [
         {
          "active": 0,
          "currentvalue": {
           "prefix": "Year: ",
           "visible": true,
           "xanchor": "center"
          },
          "len": 0.9,
          "pad": {
           "b": 10,
           "t": 50
          },
          "steps": [
           {
            "args": [
             {
              "visible": [
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false
              ]
             }
            ],
            "label": "2022",
            "method": "update"
           },
           {
            "args": [
             {
              "visible": [
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false
              ]
             }
            ],
            "label": "2023",
            "method": "update"
           },
           {
            "args": [
             {
              "visible": [
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false
              ]
             }
            ],
            "label": "2024",
            "method": "update"
           },
           {
            "args": [
             {
              "visible": [
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               false,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true,
               true
              ]
             }
            ],
            "label": "2025",
            "method": "update"
           }
          ],
          "x": 0,
          "xanchor": "left",
          "y": -0.1,
          "yanchor": "top"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "text": "Multi-label Clusters for each Year"
        },
        "width": 900,
        "xaxis": {
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "showgrid": true,
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "gridcolor": "lightgray",
         "range": [
          -4.5,
          4.5
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "showgrid": true,
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create figure\n",
    "slider_fig = go.Figure()\n",
    "\n",
    "# Add invisible legend traces first\n",
    "for label in unique_labels:\n",
    "    slider_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],  # No actual data\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=label_colors[label],\n",
    "                line=dict(color='black', width=1)\n",
    "            ),\n",
    "            legendgroup=label,\n",
    "            showlegend=True,\n",
    "            name=label\n",
    "        )\n",
    "    )\n",
    "\n",
    "num_legend_traces = len(unique_labels)\n",
    "\n",
    "# Add traces for each label and year combination\n",
    "for year in unique_years:\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, lbls in enumerate(labels_list) if label in lbls and years[i] == year]\n",
    "        \n",
    "        if len(indices) > 0:\n",
    "            cluster_points = positions[indices]\n",
    "            \n",
    "            # Create hover text\n",
    "            hover_texts = []\n",
    "            for i in indices:\n",
    "                paper = \"<br>\".join(wrap(df.iloc[i]['title'], width=50))\n",
    "                all_labels = ', '.join(labels_list[i])\n",
    "                paper_id = df.iloc[i]['ID']\n",
    "                hover_texts.append(f\"Paper: {paper}<br>ID: {paper_id}<br>Subtopics: {all_labels}<br>Year: {year}\")\n",
    "            \n",
    "            slider_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=cluster_points[:, 0],\n",
    "                    y=cluster_points[:, 1],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=label_colors[label],\n",
    "                        opacity=0.7,\n",
    "                        line=dict(color='black', width=1)\n",
    "                    ),\n",
    "                    name=f'{label}',\n",
    "                    legendgroup=label,\n",
    "                    showlegend=False,  # Don't show in legend (using invisible traces)\n",
    "                    hovertext=hover_texts,\n",
    "                    hoverinfo='text',\n",
    "                    visible=False  # Start with all hidden\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Create slider steps\n",
    "steps = []\n",
    "\n",
    "# Build visibility mapping for each year\n",
    "trace_idx = num_legend_traces  # Start after legend traces\n",
    "year_trace_map = {}  # Maps year to list of trace indices\n",
    "\n",
    "for year in unique_years:\n",
    "    year_trace_map[year] = []\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, lbls in enumerate(labels_list) if label in lbls and years[i] == year]\n",
    "        if len(indices) > 0:\n",
    "            year_trace_map[year].append(trace_idx)\n",
    "            trace_idx += 1\n",
    "\n",
    "# Add step for each year\n",
    "for year in unique_years:\n",
    "    # Create visibility array: legend traces always visible, data traces only for this year\n",
    "    visible = [True] * num_legend_traces  # Legend traces always visible\n",
    "    \n",
    "    # Add visibility for all data traces\n",
    "    total_data_traces = len(slider_fig.data) - num_legend_traces\n",
    "    visible.extend([False] * total_data_traces)\n",
    "    \n",
    "    # Set visible=True only for this year's traces\n",
    "    for trace_idx in year_trace_map[year]:\n",
    "        visible[trace_idx] = True\n",
    "    \n",
    "    steps.append(dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": visible},\n",
    "            #   {\"title\": f\"Multi-label Clusters - Year {year}\"}\n",
    "        ],\n",
    "        label=str(year)\n",
    "    ))\n",
    "\n",
    "if len(steps) > 0:\n",
    "    first_step_visible = steps[0]['args'][0]['visible']\n",
    "    for i, trace in enumerate(slider_fig.data):\n",
    "        trace.visible = first_step_visible[i]\n",
    "\n",
    "# # Add \"All Years\" as first step\n",
    "# all_visible = [True] * len(slider_fig.data)\n",
    "# steps.insert(0, dict(\n",
    "#     method=\"update\",\n",
    "#     args=[{\"visible\": all_visible},\n",
    "#           {\"title\": \"Multi-label Clusters (All Years)\"}],\n",
    "#     label=\"All\"\n",
    "# ))\n",
    "\n",
    "# Create slider\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "    yanchor=\"top\",\n",
    "    y=-0.1,\n",
    "    xanchor=\"left\",\n",
    "    x=0.0,\n",
    "    currentvalue=dict(\n",
    "        prefix=\"Year: \",\n",
    "        visible=True,\n",
    "        xanchor=\"center\"\n",
    "    ),\n",
    "    pad=dict(b=10, t=50),\n",
    "    len=0.9,\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "# Update layout with slider\n",
    "slider_fig.update_layout(\n",
    "    height=700,\n",
    "    width=900,\n",
    "    title_text=\"Multi-label Clusters for each Year\",\n",
    "    title_font_size=16,\n",
    "    hovermode='closest',\n",
    "    template='plotly_white',\n",
    "    sliders=sliders,\n",
    "    xaxis=dict(\n",
    "        range=[-4.5, 4.5],\n",
    "        title='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[-4.5, 4.5],\n",
    "        title='',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1\n",
    "    ),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "slider_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8ceb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_html_path=r\"./docs/index.html\"\n",
    "input_template_path = r\"./docs/template_file.html\"\n",
    "plotly_jinja_data = {\"all_fig\": all_fig.to_html(full_html=False, include_plotlyjs='cdn'),\n",
    "                     \"slider_fig\": slider_fig.to_html(full_html=False, include_plotlyjs='cdn')}\n",
    "#consider also defining the include_plotlyjs parameter to point to an external Plotly.js as described above\n",
    "\n",
    "with open(output_html_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    with open(input_template_path) as template_file:\n",
    "        j2_template = Template(template_file.read())\n",
    "        output_file.write(j2_template.render(plotly_jinja_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "325ec892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_fig.write_html(\"all_fig.html\")\n",
    "# slider_fig.write_html(\"slider_fig.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e382094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Source - https://stackoverflow.com/a\n",
    "# # Posted by Milos K, modified by community. See post 'Timeline' for change history\n",
    "# # Retrieved 2026-01-28, License - CC BY-SA 4.0\n",
    "\n",
    "# with open('index.html', 'a') as f:\n",
    "#     f.write('Visualisation of Alignment Survey')\n",
    "#     f.write(all_fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "#     f.write(slider_fig.to_html(full_html=False, include_plotlyjs=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
